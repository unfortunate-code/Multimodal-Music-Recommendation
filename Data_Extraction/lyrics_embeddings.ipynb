{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0763e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 18:41:12.359016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-21 18:41:12.359033: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package punkt to /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d30254",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/lyrics/merged_lyrics.pkl', 'rb') as file:\n",
    "    lyrics_dict = pickle.load(file)\n",
    "with open('../Data/song_spotify_id_mapping.pkl', 'rb') as file:\n",
    "    song_id_map = pickle.load(file)\n",
    "lyrics_id_map = {}\n",
    "for song, id_ in song_id_map.items():\n",
    "    if song in lyrics_dict:\n",
    "        lyrics_id_map[id_] = ' '.join(lyrics_dict[song].replace('\\r', '\\n').split('\\n')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcb0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, lyrics in lyrics_id_map.items():\n",
    "    lyrics = lyrics.lower().replace('-', ' ')\n",
    "    lyrics = lyrics[:-5]\n",
    "    ord0 = ord('0')\n",
    "    for i in range(len(lyrics) - 1, - 1, -1):\n",
    "        if ord(lyrics[i]) - ord0 > 9 or ord(lyrics[i]) < ord0:\n",
    "            break\n",
    "    lyrics = lyrics[:i + 1]\n",
    "    def add_g(token):\n",
    "        if token[-1] == \"'\":\n",
    "            token = token[:-1] + 'g'\n",
    "        return token\n",
    "    lyrics = ' '.join([add_g(token) for token in lyrics.split()])\n",
    "    tokens = nltk.word_tokenize(lyrics)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    lyrics = ' '.join(tokens)\n",
    "    lyrics_id_map[id_] = lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "645f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "miniLM_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01c3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "miniLM_sentence_embeddings = miniLM_model.encode(list(lyrics_id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "273f59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/embeddings/lyrics_embeddings_minLM.pkl', 'wb') as file:\n",
    "    keys = list(lyrics_id_map.keys())\n",
    "    pickle.dump({keys[i]: miniLM_sentence_embeddings[i] for i in range(len(keys))}, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccea39d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for '/home/anirudh/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/pytorch_model.bin' at '/home/anirudh/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:349\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:527\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:224\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_reader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1579022027550/work/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /opt/conda/conda-bld/pytorch_1579022027550/work/caffe2/serialize/inline_container.cc:132)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7f540abc8627 in /home/anirudh/miniconda3/lib/python3.8/site-packages/torch/lib/libc10.so)\nframe #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1f5b (0x7f54144ca9ab in /home/anirudh/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch.so)\nframe #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::string const&) + 0x64 (0x7f54144cbbc4 in /home/anirudh/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x6ce4b6 (0x7f5441f124b6 in /home/anirudh/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\nframe #4: <unknown function> + 0x28b8a7 (0x7f5441acf8a7 in /home/anirudh/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\nframe #5: <unknown function> + 0x13c7ae (0x55f2d10187ae in /home/anirudh/miniconda3/bin/python)\nframe #6: _PyObject_MakeTpCall + 0x3bf (0x55f2d100d25f in /home/anirudh/miniconda3/bin/python)\nframe #7: <unknown function> + 0x166d6a (0x55f2d1042d6a in /home/anirudh/miniconda3/bin/python)\nframe #8: PyObject_Call + 0x7d (0x55f2d101357d in /home/anirudh/miniconda3/bin/python)\nframe #9: <unknown function> + 0xdc689 (0x55f2d0fb8689 in /home/anirudh/miniconda3/bin/python)\nframe #10: _PyObject_MakeTpCall + 0x228 (0x55f2d100d0c8 in /home/anirudh/miniconda3/bin/python)\nframe #11: _PyEval_EvalFrameDefault + 0x5437 (0x55f2d10b6e87 in /home/anirudh/miniconda3/bin/python)\nframe #12: _PyEval_EvalCodeWithName + 0x888 (0x55f2d10a8818 in /home/anirudh/miniconda3/bin/python)\nframe #13: _PyFunction_Vectorcall + 0x594 (0x55f2d10a97b4 in /home/anirudh/miniconda3/bin/python)\nframe #14: <unknown function> + 0x1b9318 (0x55f2d1095318 in /home/anirudh/miniconda3/bin/python)\nframe #15: _PyObject_MakeTpCall + 0x228 (0x55f2d100d0c8 in /home/anirudh/miniconda3/bin/python)\nframe #16: _PyEval_EvalFrameDefault + 0x4ef0 (0x55f2d10b6940 in /home/anirudh/miniconda3/bin/python)\nframe #17: _PyEval_EvalCodeWithName + 0x260 (0x55f2d10a81f0 in /home/anirudh/miniconda3/bin/python)\nframe #18: _PyFunction_Vectorcall + 0x594 (0x55f2d10a97b4 in /home/anirudh/miniconda3/bin/python)\nframe #19: _PyEval_EvalFrameDefault + 0x1517 (0x55f2d10b2f67 in /home/anirudh/miniconda3/bin/python)\nframe #20: _PyFunction_Vectorcall + 0x1b7 (0x55f2d10a93d7 in /home/anirudh/miniconda3/bin/python)\nframe #21: _PyEval_EvalFrameDefault + 0x71a (0x55f2d10b216a in /home/anirudh/miniconda3/bin/python)\nframe #22: _PyEval_EvalCodeWithName + 0x260 (0x55f2d10a81f0 in /home/anirudh/miniconda3/bin/python)\nframe #23: _PyFunction_Vectorcall + 0x594 (0x55f2d10a97b4 in /home/anirudh/miniconda3/bin/python)\nframe #24: <unknown function> + 0x166ca8 (0x55f2d1042ca8 in /home/anirudh/miniconda3/bin/python)\nframe #25: PyObject_Call + 0x319 (0x55f2d1013819 in /home/anirudh/miniconda3/bin/python)\nframe #26: _PyEval_EvalFrameDefault + 0x1dd3 (0x55f2d10b3823 in /home/anirudh/miniconda3/bin/python)\nframe #27: _PyEval_EvalCodeWithName + 0x260 (0x55f2d10a81f0 in /home/anirudh/miniconda3/bin/python)\nframe #28: _PyFunction_Vectorcall + 0x594 (0x55f2d10a97b4 in /home/anirudh/miniconda3/bin/python)\nframe #29: <unknown function> + 0x166bde (0x55f2d1042bde in /home/anirudh/miniconda3/bin/python)\nframe #30: _PyEval_EvalFrameDefault + 0x1517 (0x55f2d10b2f67 in /home/anirudh/miniconda3/bin/python)\nframe #31: _PyFunction_Vectorcall + 0x1b7 (0x55f2d10a93d7 in /home/anirudh/miniconda3/bin/python)\nframe #32: <unknown function> + 0x166bde (0x55f2d1042bde in /home/anirudh/miniconda3/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x4f81 (0x55f2d10b69d1 in /home/anirudh/miniconda3/bin/python)\nframe #34: _PyEval_EvalCodeWithName + 0x888 (0x55f2d10a8818 in /home/anirudh/miniconda3/bin/python)\nframe #35: _PyFunction_Vectorcall + 0x594 (0x55f2d10a97b4 in /home/anirudh/miniconda3/bin/python)\nframe #36: <unknown function> + 0x1b945a (0x55f2d109545a in /home/anirudh/miniconda3/bin/python)\nframe #37: <unknown function> + 0x13c6c7 (0x55f2d10186c7 in /home/anirudh/miniconda3/bin/python)\nframe #38: PyObject_Call + 0x45d (0x55f2d101395d in /home/anirudh/miniconda3/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x1dd3 (0x55f2d10b3823 in /home/anirudh/miniconda3/bin/python)\nframe #40: _PyFunction_Vectorcall + 0x1b7 (0x55f2d10a93d7 in /home/anirudh/miniconda3/bin/python)\nframe #41: _PyEval_EvalFrameDefault + 0x4f81 (0x55f2d10b69d1 in /home/anirudh/miniconda3/bin/python)\nframe #42: _PyFunction_Vectorcall + 0x1b7 (0x55f2d10a93d7 in /home/anirudh/miniconda3/bin/python)\nframe #43: <unknown function> + 0x166bde (0x55f2d1042bde in /home/anirudh/miniconda3/bin/python)\nframe #44: _PyEval_EvalFrameDefault + 0x4f81 (0x55f2d10b69d1 in /home/anirudh/miniconda3/bin/python)\nframe #45: _PyEval_EvalCodeWithName + 0x888 (0x55f2d10a8818 in /home/anirudh/miniconda3/bin/python)\nframe #46: _PyFunction_Vectorcall + 0x594 (0x55f2d10a97b4 in /home/anirudh/miniconda3/bin/python)\nframe #47: <unknown function> + 0x1b9318 (0x55f2d1095318 in /home/anirudh/miniconda3/bin/python)\nframe #48: _PyObject_MakeTpCall + 0x228 (0x55f2d100d0c8 in /home/anirudh/miniconda3/bin/python)\nframe #49: _PyEval_EvalFrameDefault + 0x4ef0 (0x55f2d10b6940 in /home/anirudh/miniconda3/bin/python)\nframe #50: _PyEval_EvalCodeWithName + 0x260 (0x55f2d10a81f0 in /home/anirudh/miniconda3/bin/python)\nframe #51: <unknown function> + 0x1f722f (0x55f2d10d322f in /home/anirudh/miniconda3/bin/python)\nframe #52: <unknown function> + 0x13bb0d (0x55f2d1017b0d in /home/anirudh/miniconda3/bin/python)\nframe #53: _PyEval_EvalFrameDefault + 0x71a (0x55f2d10b216a in /home/anirudh/miniconda3/bin/python)\nframe #54: <unknown function> + 0x195462 (0x55f2d1071462 in /home/anirudh/miniconda3/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x1b5c (0x55f2d10b35ac in /home/anirudh/miniconda3/bin/python)\nframe #56: <unknown function> + 0x195462 (0x55f2d1071462 in /home/anirudh/miniconda3/bin/python)\nframe #57: _PyEval_EvalFrameDefault + 0x1b5c (0x55f2d10b35ac in /home/anirudh/miniconda3/bin/python)\nframe #58: <unknown function> + 0x195462 (0x55f2d1071462 in /home/anirudh/miniconda3/bin/python)\nframe #59: <unknown function> + 0x17cc45 (0x55f2d1058c45 in /home/anirudh/miniconda3/bin/python)\nframe #60: _PyEval_EvalFrameDefault + 0x4bf (0x55f2d10b1f0f in /home/anirudh/miniconda3/bin/python)\nframe #61: _PyFunction_Vectorcall + 0x1b7 (0x55f2d10a93d7 in /home/anirudh/miniconda3/bin/python)\nframe #62: _PyEval_EvalFrameDefault + 0x71a (0x55f2d10b216a in /home/anirudh/miniconda3/bin/python)\nframe #63: _PyFunction_Vectorcall + 0x1b7 (0x55f2d10a93d7 in /home/anirudh/miniconda3/bin/python)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:353\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mpnet_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:94\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     86\u001b[0m     snapshot_download(model_name_or_path,\n\u001b[1;32m     87\u001b[0m                         cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[1;32m     88\u001b[0m                         library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     89\u001b[0m                         library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[1;32m     90\u001b[0m                         ignore_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflax_model.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrust_model.ot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     91\u001b[0m                         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:831\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_config \u001b[38;5;129;01min\u001b[39;00m modules_config:\n\u001b[1;32m    830\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m import_from_string(module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 831\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m     modules[module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sbert_config_path) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[1;32m    136\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fIn)\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:446\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    445\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:1797\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded:\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 1797\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;66;03m#    weights entry - we assume all weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:365\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    361\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/home/anirudh/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/pytorch_model.bin' at '/home/anirudh/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "mpnet_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_sentence_embeddings = mpnet_model.encode(list(lyrics_id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8998225",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/embeddings/lyrics_embeddings_mpnet.pkl', 'wb') as file:\n",
    "    keys = list(lyrics_id_map.keys())\n",
    "    pickle.dump({keys[i]: mpnet_sentence_embeddings[i] for i in range(len(keys))}, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad9221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim.downloader as api\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "glove_model300 = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b94b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[text for text in lyrics.split()] for lyrics in lyrics_id_map.values()]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d916e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word2vec_sentence_embeddings = np.zeros((len(lyrics_id_map.values()), 300), dtype=np.float32)\n",
    "for i, lyrics in enumerate(lyrics_id_map.values()):\n",
    "    count = 0\n",
    "    for token in lyrics.split():\n",
    "        if token in word2vec_model300:\n",
    "            count += 1\n",
    "            word2vec_sentence_embeddings[i] += word2vec_model300[token]\n",
    "    if count > 0:\n",
    "        word2vec_sentence_embeddings[i] /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b383267",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/embeddings/lyrics_embeddings_word2_vec.pkl', 'wb') as file:\n",
    "    keys = list(lyrics_id_map.keys())\n",
    "    pickle.dump({keys[i]: word2vec_sentence_embeddings[i] for i in range(len(keys))}, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8abb93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_sentence_embeddings = np.zeros((len(lyrics_id_map.values()), 300), dtype=np.float32)\n",
    "for i, lyrics in enumerate(lyrics_id_map.values()):\n",
    "    count = 0\n",
    "    for token in lyrics.split():\n",
    "        if token in glove_model300:\n",
    "            count += 1\n",
    "            glove_sentence_embeddings[i] += glove_model300[token]\n",
    "    if count > 0:\n",
    "        glove_sentence_embeddings[i] /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e76aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/embeddings/lyrics_embeddings_glove.pkl', 'wb') as file:\n",
    "    keys = list(lyrics_id_map.keys())\n",
    "    pickle.dump({keys[i]: glove_sentence_embeddings[i] for i in range(len(keys))}, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39374c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5679/2804340455.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  word2vec_trained_embeddings[i] /= len(lyrics.split())\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(texts, min_count = 1, vector_size=300)\n",
    "word2vec_trained_embeddings = np.zeros((len(lyrics_id_map.values()), 300), dtype=np.float32)\n",
    "for i, lyrics in enumerate(lyrics_id_map.values()):\n",
    "    for token in lyrics.split():\n",
    "        word2vec_trained_embeddings[i] += model.wv[token]\n",
    "    word2vec_trained_embeddings[i] /= len(lyrics.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56f81bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/embeddings/lyrics_embeddings_word2vec_trained.pkl', 'wb') as file:\n",
    "    keys = list(lyrics_id_map.keys())\n",
    "    pickle.dump({keys[i]: word2vec_trained_embeddings[i] for i in range(len(keys))}, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
