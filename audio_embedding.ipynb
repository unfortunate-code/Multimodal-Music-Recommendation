{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIXzzB1ORwv2"
      },
      "source": [
        "## Audio Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fYPqpcjyNvxJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "\n",
        "class LinearVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=150, beta=0):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.beta = beta\n",
        "        \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        " \n",
        "    def forward(self, x):\n",
        "        z, mu, log_var = self.encode(x) # encoding\n",
        "        reconstruction = self.decoder(z) # decoding\n",
        "        return reconstruction, mu, log_var\n",
        "    \n",
        "    def encode(self, x):\n",
        "        q = self.encoder(x)\n",
        "        q = q.reshape(-1, 2, self.latent_dim)\n",
        "        mu = q[:, 0, :] # the first feature values as mean\n",
        "        log_var = q[:, 1, :] # the other feature values as variance\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return z, mu, log_var\n",
        "    \n",
        "    def loss(self, x, rec, mu, log_var):\n",
        "        # compute reconstruction loss\n",
        "        rec_loss = self.criterion(x, rec)\n",
        "        # compute KL divergence loss\n",
        "        log_sigma = 0.5*log_var\n",
        "        mu_unit = torch.zeros_like(mu)\n",
        "        log_sigma_unit = torch.zeros_like(log_sigma)\n",
        "        kl_loss = kl_divergence(mu, log_sigma, mu_unit, log_sigma_unit)\n",
        "        kl_loss = torch.sum(kl_loss,axis=1) # sum across the latent dimension, not the batch dimension\n",
        "        kl_loss = torch.mean(kl_loss) # make sure that this is a scalar, not a vector / array \n",
        "\n",
        "        return rec_loss + self.beta * kl_loss, {'rec_loss': rec_loss.cpu().detach().numpy(), 'kl_loss': kl_loss.cpu().detach().numpy()}\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=150):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=11, out_features=100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=100, out_features=100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=100, out_features=latent_dim*2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x) \n",
        "    \n",
        " \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=150):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_dim, out_features=100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=100, out_features=100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=100, out_features=11),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x) \n",
        "    \n",
        "def kl_divergence(mu1, log_sigma1, mu2, log_sigma2):\n",
        "  \"\"\"Computes KL[p||q] between two Gaussians defined by [mu, log_sigma].\"\"\"\n",
        "  return (log_sigma2 - log_sigma1) + (torch.exp(log_sigma1) ** 2 + (mu1 - mu2) ** 2) \\\n",
        "               / (2 * torch.exp(log_sigma2) ** 2) - 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4qoB3mR5tV"
      },
      "source": [
        "### Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nVkovbIzNx1p"
      },
      "outputs": [],
      "source": [
        "feature_columns = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
        "    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "\n",
        "def load_data(filepaths):\n",
        "    data = []\n",
        "    for filepath in filepaths:\n",
        "        print(f'loading {filepath}..')\n",
        "        # df = pd.read_csv('Data/audio/audio_features.txt',sep='\\t')\n",
        "        df = pd.read_csv(filepath,sep='\\t')\n",
        "        data.append(df)\n",
        "    data = pd.concat(data).drop_duplicates()\n",
        "    return data\n",
        "\n",
        "def preprocessing_features(data):    \n",
        "    feats = np.array(data[feature_columns],dtype=float)\n",
        "    # features = (features - np.mean(features, axis=0))/np.std(features, axis=0)\n",
        "    feats = (feats - np.min(feats, axis=0))/np.ptp(feats, axis=0)\n",
        "    print(feats.shape, feats.dtype)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pRuleuBDR_Ak"
      },
      "outputs": [],
      "source": [
        "filepaths = ['audio_features.txt', 'audio_features2.txt', 'audio_features3.txt']\n",
        "model_path = 'models/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0OuTPjHuOP96",
        "outputId": "5a6b07f0-0d46-45a0-cdf8-aee2dec9ccc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading audio_features.txt..\n",
            "loading audio_features2.txt..\n",
            "loading audio_features3.txt..\n",
            "(174506, 11) float64\n",
            "cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "It 0: Total Loss: 0.11547081172466278, \t Rec Loss: 0.11462929099798203,\t KL Loss: 0.8415201902389526\n",
            "It 100: Total Loss: 0.04320327937602997, \t Rec Loss: 0.03847368806600571,\t KL Loss: 4.729591369628906\n",
            "It 200: Total Loss: 0.02804698795080185, \t Rec Loss: 0.019029593095183372,\t KL Loss: 9.01739501953125\n",
            "It 300: Total Loss: 0.026251042261719704, \t Rec Loss: 0.017772559076547623,\t KL Loss: 8.478483200073242\n",
            "It 400: Total Loss: 0.0231500044465065, \t Rec Loss: 0.015054198913276196,\t KL Loss: 8.095804214477539\n",
            "It 500: Total Loss: 0.024560749530792236, \t Rec Loss: 0.016720639541745186,\t KL Loss: 7.840109825134277\n",
            "It 600: Total Loss: 0.024955380707979202, \t Rec Loss: 0.016731757670640945,\t KL Loss: 8.223621368408203\n",
            "It 700: Total Loss: 0.02462039887905121, \t Rec Loss: 0.016472183167934418,\t KL Loss: 8.148216247558594\n",
            "It 800: Total Loss: 0.02466500923037529, \t Rec Loss: 0.015490460209548473,\t KL Loss: 9.17454719543457\n",
            "It 900: Total Loss: 0.02359658107161522, \t Rec Loss: 0.0146433524787426,\t KL Loss: 8.953227043151855\n",
            "It 1000: Total Loss: 0.023679599165916443, \t Rec Loss: 0.015015124343335629,\t KL Loss: 8.664474487304688\n",
            "It 1100: Total Loss: 0.024512914940714836, \t Rec Loss: 0.01414845883846283,\t KL Loss: 10.364455223083496\n",
            "It 1200: Total Loss: 0.021416835486888885, \t Rec Loss: 0.011970140039920807,\t KL Loss: 9.446694374084473\n",
            "It 1300: Total Loss: 0.02229667268693447, \t Rec Loss: 0.013146390207111835,\t KL Loss: 9.15028190612793\n",
            "It 1400: Total Loss: 0.02200263738632202, \t Rec Loss: 0.012633617036044598,\t KL Loss: 9.3690185546875\n",
            "It 1500: Total Loss: 0.017886191606521606, \t Rec Loss: 0.009374127723276615,\t KL Loss: 8.512063980102539\n",
            "It 1600: Total Loss: 0.01785307750105858, \t Rec Loss: 0.009613627567887306,\t KL Loss: 8.239450454711914\n",
            "It 1700: Total Loss: 0.01792379841208458, \t Rec Loss: 0.009104934521019459,\t KL Loss: 8.818864822387695\n",
            "It 1800: Total Loss: 0.01711624301970005, \t Rec Loss: 0.009429143741726875,\t KL Loss: 7.687098503112793\n",
            "It 1900: Total Loss: 0.019225895404815674, \t Rec Loss: 0.011008509434759617,\t KL Loss: 8.217384338378906\n",
            "It 2000: Total Loss: 0.017260562628507614, \t Rec Loss: 0.009858253411948681,\t KL Loss: 7.402309894561768\n",
            "It 2100: Total Loss: 0.016869129613041878, \t Rec Loss: 0.009578341618180275,\t KL Loss: 7.290788173675537\n",
            "It 2200: Total Loss: 0.015473753213882446, \t Rec Loss: 0.007887103594839573,\t KL Loss: 7.5866498947143555\n",
            "It 2300: Total Loss: 0.01825302466750145, \t Rec Loss: 0.010341301560401917,\t KL Loss: 7.911723613739014\n",
            "It 2400: Total Loss: 0.016541965305805206, \t Rec Loss: 0.009028111584484577,\t KL Loss: 7.513853073120117\n",
            "It 2500: Total Loss: 0.01660134829580784, \t Rec Loss: 0.008964084088802338,\t KL Loss: 7.637263298034668\n",
            "It 2600: Total Loss: 0.01731007546186447, \t Rec Loss: 0.008518611080944538,\t KL Loss: 8.791463851928711\n",
            "It 2700: Total Loss: 0.015883073210716248, \t Rec Loss: 0.007824845612049103,\t KL Loss: 8.0582275390625\n",
            "Epoch 2/20\n",
            "It 2800: Total Loss: 0.01574738696217537, \t Rec Loss: 0.008306020870804787,\t KL Loss: 7.4413652420043945\n",
            "It 2900: Total Loss: 0.01615547202527523, \t Rec Loss: 0.008914126083254814,\t KL Loss: 7.241345405578613\n",
            "It 3000: Total Loss: 0.015149611048400402, \t Rec Loss: 0.008037539198994637,\t KL Loss: 7.112071514129639\n",
            "It 3100: Total Loss: 0.015534547157585621, \t Rec Loss: 0.008464264683425426,\t KL Loss: 7.070281982421875\n",
            "It 3200: Total Loss: 0.014483332633972168, \t Rec Loss: 0.006582231726497412,\t KL Loss: 7.901099681854248\n",
            "It 3300: Total Loss: 0.01533545833081007, \t Rec Loss: 0.007624299265444279,\t KL Loss: 7.711158752441406\n",
            "It 3400: Total Loss: 0.01654718443751335, \t Rec Loss: 0.008873899467289448,\t KL Loss: 7.673283576965332\n",
            "It 3500: Total Loss: 0.01538829505443573, \t Rec Loss: 0.008021123707294464,\t KL Loss: 7.367170333862305\n",
            "It 3600: Total Loss: 0.014713482931256294, \t Rec Loss: 0.007307815365493298,\t KL Loss: 7.405667304992676\n",
            "It 3700: Total Loss: 0.015201812610030174, \t Rec Loss: 0.008017607033252716,\t KL Loss: 7.184205055236816\n",
            "It 3800: Total Loss: 0.0149082625284791, \t Rec Loss: 0.007880046032369137,\t KL Loss: 7.028216361999512\n",
            "It 3900: Total Loss: 0.01650344580411911, \t Rec Loss: 0.00880331639200449,\t KL Loss: 7.700128078460693\n",
            "It 4000: Total Loss: 0.014302054420113564, \t Rec Loss: 0.007002294063568115,\t KL Loss: 7.299760341644287\n",
            "It 4100: Total Loss: 0.013428030535578728, \t Rec Loss: 0.005956756882369518,\t KL Loss: 7.471273422241211\n",
            "It 4200: Total Loss: 0.015222903341054916, \t Rec Loss: 0.0075761377811431885,\t KL Loss: 7.64676570892334\n",
            "It 4300: Total Loss: 0.014245972037315369, \t Rec Loss: 0.006873908452689648,\t KL Loss: 7.372063636779785\n",
            "It 4400: Total Loss: 0.013987705111503601, \t Rec Loss: 0.006620078347623348,\t KL Loss: 7.367626190185547\n",
            "It 4500: Total Loss: 0.013897299766540527, \t Rec Loss: 0.006677308585494757,\t KL Loss: 7.2199907302856445\n",
            "It 4600: Total Loss: 0.012977370992302895, \t Rec Loss: 0.005474382545799017,\t KL Loss: 7.502988338470459\n",
            "It 4700: Total Loss: 0.015037532895803452, \t Rec Loss: 0.007619536481797695,\t KL Loss: 7.417995452880859\n",
            "It 4800: Total Loss: 0.014571180567145348, \t Rec Loss: 0.006999870762228966,\t KL Loss: 7.571310043334961\n",
            "It 4900: Total Loss: 0.01367837842553854, \t Rec Loss: 0.006097931880503893,\t KL Loss: 7.580446243286133\n",
            "It 5000: Total Loss: 0.014353564940392971, \t Rec Loss: 0.006603619083762169,\t KL Loss: 7.749945640563965\n",
            "It 5100: Total Loss: 0.012667549774050713, \t Rec Loss: 0.005193243268877268,\t KL Loss: 7.474306106567383\n",
            "It 5200: Total Loss: 0.014575010165572166, \t Rec Loss: 0.006984047591686249,\t KL Loss: 7.5909624099731445\n",
            "It 5300: Total Loss: 0.013247312977910042, \t Rec Loss: 0.0056093595921993256,\t KL Loss: 7.6379523277282715\n",
            "It 5400: Total Loss: 0.014980807900428772, \t Rec Loss: 0.007434491533786058,\t KL Loss: 7.546316623687744\n",
            "Epoch 3/20\n",
            "It 5500: Total Loss: 0.013551134616136551, \t Rec Loss: 0.005615320522338152,\t KL Loss: 7.9358134269714355\n",
            "It 5600: Total Loss: 0.014236675575375557, \t Rec Loss: 0.006694249343127012,\t KL Loss: 7.542425632476807\n",
            "It 5700: Total Loss: 0.01312679797410965, \t Rec Loss: 0.00541315833106637,\t KL Loss: 7.713639259338379\n",
            "It 5800: Total Loss: 0.013860458508133888, \t Rec Loss: 0.006491790059953928,\t KL Loss: 7.368668079376221\n",
            "It 5900: Total Loss: 0.013573287054896355, \t Rec Loss: 0.0060440776869654655,\t KL Loss: 7.529209613800049\n",
            "It 6000: Total Loss: 0.012863440439105034, \t Rec Loss: 0.005378681235015392,\t KL Loss: 7.484759330749512\n",
            "It 6100: Total Loss: 0.012837659567594528, \t Rec Loss: 0.005117493215948343,\t KL Loss: 7.720165729522705\n",
            "It 6200: Total Loss: 0.0122795719653368, \t Rec Loss: 0.004905174486339092,\t KL Loss: 7.3743977546691895\n",
            "It 6300: Total Loss: 0.013626081869006157, \t Rec Loss: 0.0057084993459284306,\t KL Loss: 7.917582988739014\n",
            "It 6400: Total Loss: 0.012921342626214027, \t Rec Loss: 0.005030258093029261,\t KL Loss: 7.891083717346191\n",
            "It 6500: Total Loss: 0.013396019116044044, \t Rec Loss: 0.005136413499712944,\t KL Loss: 8.259605407714844\n",
            "It 6600: Total Loss: 0.013466103002429008, \t Rec Loss: 0.005957385059446096,\t KL Loss: 7.5087175369262695\n",
            "It 6700: Total Loss: 0.013425331562757492, \t Rec Loss: 0.00574022950604558,\t KL Loss: 7.685101509094238\n",
            "It 6800: Total Loss: 0.013978946022689342, \t Rec Loss: 0.0056217145174741745,\t KL Loss: 8.357231140136719\n",
            "It 6900: Total Loss: 0.013571655377745628, \t Rec Loss: 0.006019332446157932,\t KL Loss: 7.552322864532471\n",
            "It 7000: Total Loss: 0.013681886717677116, \t Rec Loss: 0.005441159941256046,\t KL Loss: 8.240726470947266\n",
            "It 7100: Total Loss: 0.012490080669522285, \t Rec Loss: 0.005211053881794214,\t KL Loss: 7.279026985168457\n",
            "It 7200: Total Loss: 0.012575468048453331, \t Rec Loss: 0.00481219869107008,\t KL Loss: 7.763269424438477\n",
            "It 7300: Total Loss: 0.013102997094392776, \t Rec Loss: 0.005049117840826511,\t KL Loss: 8.053878784179688\n",
            "It 7400: Total Loss: 0.012991029769182205, \t Rec Loss: 0.005064839962869883,\t KL Loss: 7.926189422607422\n",
            "It 7500: Total Loss: 0.012572942301630974, \t Rec Loss: 0.0050621903501451015,\t KL Loss: 7.510751724243164\n",
            "It 7600: Total Loss: 0.014012889936566353, \t Rec Loss: 0.005462659057229757,\t KL Loss: 8.550230026245117\n",
            "It 7700: Total Loss: 0.013832145370543003, \t Rec Loss: 0.005600146949291229,\t KL Loss: 8.231998443603516\n",
            "It 7800: Total Loss: 0.013066806830465794, \t Rec Loss: 0.004881847649812698,\t KL Loss: 8.184958457946777\n",
            "It 7900: Total Loss: 0.01321394182741642, \t Rec Loss: 0.0054717231541872025,\t KL Loss: 7.742218971252441\n",
            "It 8000: Total Loss: 0.01236401405185461, \t Rec Loss: 0.004604874644428492,\t KL Loss: 7.759139060974121\n",
            "It 8100: Total Loss: 0.01243614498525858, \t Rec Loss: 0.004941471852362156,\t KL Loss: 7.494672775268555\n",
            "Epoch 4/20\n",
            "It 8200: Total Loss: 0.012696297839283943, \t Rec Loss: 0.005225254222750664,\t KL Loss: 7.471043586730957\n",
            "It 8300: Total Loss: 0.01411563903093338, \t Rec Loss: 0.00604812940582633,\t KL Loss: 8.067508697509766\n",
            "It 8400: Total Loss: 0.012514231726527214, \t Rec Loss: 0.004605086985975504,\t KL Loss: 7.909143447875977\n",
            "It 8500: Total Loss: 0.01213949266821146, \t Rec Loss: 0.004664184059947729,\t KL Loss: 7.475308418273926\n",
            "It 8600: Total Loss: 0.012148670852184296, \t Rec Loss: 0.004564340226352215,\t KL Loss: 7.584329605102539\n",
            "It 8700: Total Loss: 0.013162670657038689, \t Rec Loss: 0.004669852089136839,\t KL Loss: 8.492818832397461\n",
            "It 8800: Total Loss: 0.012916863895952702, \t Rec Loss: 0.004930214025080204,\t KL Loss: 7.986649036407471\n",
            "It 8900: Total Loss: 0.012507796287536621, \t Rec Loss: 0.004905913956463337,\t KL Loss: 7.601881980895996\n",
            "It 9000: Total Loss: 0.01266021654009819, \t Rec Loss: 0.004536264110356569,\t KL Loss: 8.12395191192627\n",
            "It 9100: Total Loss: 0.013406455516815186, \t Rec Loss: 0.005235154181718826,\t KL Loss: 8.171300888061523\n",
            "It 9200: Total Loss: 0.014214184135198593, \t Rec Loss: 0.005806108936667442,\t KL Loss: 8.408074378967285\n",
            "It 9300: Total Loss: 0.012701505795121193, \t Rec Loss: 0.004276860971003771,\t KL Loss: 8.424643516540527\n",
            "It 9400: Total Loss: 0.012978149577975273, \t Rec Loss: 0.005473421420902014,\t KL Loss: 7.504727363586426\n",
            "It 9500: Total Loss: 0.012671208009123802, \t Rec Loss: 0.0048726024106144905,\t KL Loss: 7.798604965209961\n",
            "It 9600: Total Loss: 0.013385679572820663, \t Rec Loss: 0.005423794966191053,\t KL Loss: 7.961884498596191\n",
            "It 9700: Total Loss: 0.015244824811816216, \t Rec Loss: 0.007489892188459635,\t KL Loss: 7.754931926727295\n",
            "It 9800: Total Loss: 0.013567738234996796, \t Rec Loss: 0.0053669908083975315,\t KL Loss: 8.200746536254883\n",
            "It 9900: Total Loss: 0.013259364292025566, \t Rec Loss: 0.005116635002195835,\t KL Loss: 8.142728805541992\n",
            "It 10000: Total Loss: 0.01257038488984108, \t Rec Loss: 0.004934883676469326,\t KL Loss: 7.635500907897949\n",
            "It 10100: Total Loss: 0.014449076727032661, \t Rec Loss: 0.005669622216373682,\t KL Loss: 8.77945327758789\n",
            "It 10200: Total Loss: 0.012426057830452919, \t Rec Loss: 0.004608817398548126,\t KL Loss: 7.817239761352539\n",
            "It 10300: Total Loss: 0.013171432539820671, \t Rec Loss: 0.005558204371482134,\t KL Loss: 7.613227844238281\n",
            "It 10400: Total Loss: 0.013297314755618572, \t Rec Loss: 0.005304631777107716,\t KL Loss: 7.992682456970215\n",
            "It 10500: Total Loss: 0.012353095225989819, \t Rec Loss: 0.004362834617495537,\t KL Loss: 7.990260124206543\n",
            "It 10600: Total Loss: 0.01280474103987217, \t Rec Loss: 0.004770517349243164,\t KL Loss: 8.034223556518555\n",
            "It 10700: Total Loss: 0.012600881978869438, \t Rec Loss: 0.004632099065929651,\t KL Loss: 7.968782424926758\n",
            "It 10800: Total Loss: 0.013514972291886806, \t Rec Loss: 0.004731141962110996,\t KL Loss: 8.783829689025879\n",
            "It 10900: Total Loss: 0.012720106169581413, \t Rec Loss: 0.004622394684702158,\t KL Loss: 8.097711563110352\n",
            "Epoch 5/20\n",
            "It 11000: Total Loss: 0.013702771626412868, \t Rec Loss: 0.005882915109395981,\t KL Loss: 7.819855690002441\n",
            "It 11100: Total Loss: 0.012072416953742504, \t Rec Loss: 0.004176043905317783,\t KL Loss: 7.8963727951049805\n",
            "It 11200: Total Loss: 0.012480797246098518, \t Rec Loss: 0.004461624659597874,\t KL Loss: 8.019172668457031\n",
            "It 11300: Total Loss: 0.012661673128604889, \t Rec Loss: 0.00449157040566206,\t KL Loss: 8.1701021194458\n",
            "It 11400: Total Loss: 0.013207229785621166, \t Rec Loss: 0.005198056809604168,\t KL Loss: 8.009172439575195\n",
            "It 11500: Total Loss: 0.012343387119472027, \t Rec Loss: 0.004662197083234787,\t KL Loss: 7.68118953704834\n",
            "It 11600: Total Loss: 0.013351934030652046, \t Rec Loss: 0.005133089143782854,\t KL Loss: 8.218844413757324\n",
            "It 11700: Total Loss: 0.012782152742147446, \t Rec Loss: 0.004620346240699291,\t KL Loss: 8.161806106567383\n",
            "It 11800: Total Loss: 0.012888828292489052, \t Rec Loss: 0.004750572144985199,\t KL Loss: 8.138256072998047\n",
            "It 11900: Total Loss: 0.012305782176554203, \t Rec Loss: 0.004701054655015469,\t KL Loss: 7.604727268218994\n",
            "It 12000: Total Loss: 0.014247972518205643, \t Rec Loss: 0.006173325702548027,\t KL Loss: 8.07464599609375\n",
            "It 12100: Total Loss: 0.012795239686965942, \t Rec Loss: 0.004623102489858866,\t KL Loss: 8.172137260437012\n",
            "It 12200: Total Loss: 0.012517903000116348, \t Rec Loss: 0.004609469790011644,\t KL Loss: 7.9084320068359375\n",
            "It 12300: Total Loss: 0.012734340503811836, \t Rec Loss: 0.004647563677281141,\t KL Loss: 8.086775779724121\n",
            "It 12400: Total Loss: 0.012345666065812111, \t Rec Loss: 0.003970845136791468,\t KL Loss: 8.3748197555542\n",
            "It 12500: Total Loss: 0.012421289458870888, \t Rec Loss: 0.004336321260780096,\t KL Loss: 8.084968566894531\n",
            "It 12600: Total Loss: 0.012991100549697876, \t Rec Loss: 0.005469061434268951,\t KL Loss: 7.522039413452148\n",
            "It 12700: Total Loss: 0.012269962579011917, \t Rec Loss: 0.004574484191834927,\t KL Loss: 7.695478439331055\n",
            "It 12800: Total Loss: 0.013037213124334812, \t Rec Loss: 0.004932587035000324,\t KL Loss: 8.104625701904297\n",
            "It 12900: Total Loss: 0.013011263683438301, \t Rec Loss: 0.004830673336982727,\t KL Loss: 8.18058967590332\n",
            "It 13000: Total Loss: 0.013330694288015366, \t Rec Loss: 0.005396752152591944,\t KL Loss: 7.933941841125488\n",
            "It 13100: Total Loss: 0.011575594544410706, \t Rec Loss: 0.0041628265753388405,\t KL Loss: 7.41276741027832\n",
            "It 13200: Total Loss: 0.012341804802417755, \t Rec Loss: 0.004777064546942711,\t KL Loss: 7.564740180969238\n",
            "It 13300: Total Loss: 0.013230346143245697, \t Rec Loss: 0.004363643936812878,\t KL Loss: 8.86670207977295\n",
            "It 13400: Total Loss: 0.013040410354733467, \t Rec Loss: 0.0048165456391870975,\t KL Loss: 8.22386360168457\n",
            "It 13500: Total Loss: 0.014606989920139313, \t Rec Loss: 0.006460806354880333,\t KL Loss: 8.146183013916016\n",
            "It 13600: Total Loss: 0.01299247145652771, \t Rec Loss: 0.004728763364255428,\t KL Loss: 8.263708114624023\n",
            "Epoch 6/20\n",
            "It 13700: Total Loss: 0.013194880448281765, \t Rec Loss: 0.005304400809109211,\t KL Loss: 7.890479564666748\n",
            "It 13800: Total Loss: 0.012312681414186954, \t Rec Loss: 0.004185476340353489,\t KL Loss: 8.127204895019531\n",
            "It 13900: Total Loss: 0.012339523993432522, \t Rec Loss: 0.004400691017508507,\t KL Loss: 7.9388322830200195\n",
            "It 14000: Total Loss: 0.012869961559772491, \t Rec Loss: 0.004374218638986349,\t KL Loss: 8.495742797851562\n",
            "It 14100: Total Loss: 0.013251672498881817, \t Rec Loss: 0.004651467315852642,\t KL Loss: 8.600204467773438\n",
            "It 14200: Total Loss: 0.014145081862807274, \t Rec Loss: 0.0054639908485114574,\t KL Loss: 8.68109130859375\n",
            "It 14300: Total Loss: 0.01327652670443058, \t Rec Loss: 0.004088108893483877,\t KL Loss: 9.188417434692383\n",
            "It 14400: Total Loss: 0.012960691004991531, \t Rec Loss: 0.004920449573546648,\t KL Loss: 8.040241241455078\n",
            "It 14500: Total Loss: 0.013062537647783756, \t Rec Loss: 0.004682824015617371,\t KL Loss: 8.37971305847168\n",
            "It 14600: Total Loss: 0.01307748630642891, \t Rec Loss: 0.005420132074505091,\t KL Loss: 7.657353401184082\n",
            "It 14700: Total Loss: 0.01269841380417347, \t Rec Loss: 0.004706029314547777,\t KL Loss: 7.9923834800720215\n",
            "It 14800: Total Loss: 0.01301872730255127, \t Rec Loss: 0.0052123465575277805,\t KL Loss: 7.806380271911621\n",
            "It 14900: Total Loss: 0.012723559513688087, \t Rec Loss: 0.0044557699002325535,\t KL Loss: 8.267789840698242\n",
            "It 15000: Total Loss: 0.013849107548594475, \t Rec Loss: 0.006034377031028271,\t KL Loss: 7.814730167388916\n",
            "It 15100: Total Loss: 0.012113156728446484, \t Rec Loss: 0.004754374269396067,\t KL Loss: 7.3587822914123535\n",
            "It 15200: Total Loss: 0.013310443609952927, \t Rec Loss: 0.005014705006033182,\t KL Loss: 8.295738220214844\n",
            "It 15300: Total Loss: 0.012696396559476852, \t Rec Loss: 0.004561698529869318,\t KL Loss: 8.134696960449219\n",
            "It 15400: Total Loss: 0.01217523030936718, \t Rec Loss: 0.004356300458312035,\t KL Loss: 7.818929195404053\n",
            "It 15500: Total Loss: 0.013921543955802917, \t Rec Loss: 0.005654871929436922,\t KL Loss: 8.266671180725098\n",
            "It 15600: Total Loss: 0.01341265719383955, \t Rec Loss: 0.005380306392908096,\t KL Loss: 8.032350540161133\n",
            "It 15700: Total Loss: 0.012986663728952408, \t Rec Loss: 0.004309908952564001,\t KL Loss: 8.67675495147705\n",
            "It 15800: Total Loss: 0.01309272926300764, \t Rec Loss: 0.005208550952374935,\t KL Loss: 7.884178161621094\n",
            "It 15900: Total Loss: 0.012654779478907585, \t Rec Loss: 0.004830623511224985,\t KL Loss: 7.824154853820801\n",
            "It 16000: Total Loss: 0.012249061837792397, \t Rec Loss: 0.004214752931147814,\t KL Loss: 8.034309387207031\n",
            "It 16100: Total Loss: 0.0119355209171772, \t Rec Loss: 0.004317028913646936,\t KL Loss: 7.618491172790527\n",
            "It 16200: Total Loss: 0.012633563950657845, \t Rec Loss: 0.004608749877661467,\t KL Loss: 8.024813652038574\n",
            "It 16300: Total Loss: 0.012688973918557167, \t Rec Loss: 0.004378422629088163,\t KL Loss: 8.310551643371582\n",
            "Epoch 7/20\n",
            "It 16400: Total Loss: 0.013231191784143448, \t Rec Loss: 0.005325422156602144,\t KL Loss: 7.905768394470215\n",
            "It 16500: Total Loss: 0.012323342263698578, \t Rec Loss: 0.004507575184106827,\t KL Loss: 7.815766334533691\n",
            "It 16600: Total Loss: 0.012975894846022129, \t Rec Loss: 0.005696529988199472,\t KL Loss: 7.279364585876465\n",
            "It 16700: Total Loss: 0.011904038488864899, \t Rec Loss: 0.0041539729572832584,\t KL Loss: 7.750065803527832\n",
            "It 16800: Total Loss: 0.012607235461473465, \t Rec Loss: 0.004172428976744413,\t KL Loss: 8.434806823730469\n",
            "It 16900: Total Loss: 0.012033438310027122, \t Rec Loss: 0.0045354184694588184,\t KL Loss: 7.498019218444824\n",
            "It 17000: Total Loss: 0.013658450916409492, \t Rec Loss: 0.0049839923158288,\t KL Loss: 8.674458503723145\n",
            "It 17100: Total Loss: 0.01259840652346611, \t Rec Loss: 0.004266338422894478,\t KL Loss: 8.332067489624023\n",
            "It 17200: Total Loss: 0.012638373300433159, \t Rec Loss: 0.004860350862145424,\t KL Loss: 7.778022766113281\n",
            "It 17300: Total Loss: 0.01188456080853939, \t Rec Loss: 0.0037308905739337206,\t KL Loss: 8.153670310974121\n",
            "It 17400: Total Loss: 0.012105433270335197, \t Rec Loss: 0.004331057891249657,\t KL Loss: 7.774374961853027\n",
            "It 17500: Total Loss: 0.011854168958961964, \t Rec Loss: 0.004223266150802374,\t KL Loss: 7.630902290344238\n",
            "It 17600: Total Loss: 0.012037507258355618, \t Rec Loss: 0.00424906937405467,\t KL Loss: 7.788437366485596\n",
            "It 17700: Total Loss: 0.012460720725357533, \t Rec Loss: 0.004368509165942669,\t KL Loss: 8.09221076965332\n",
            "It 17800: Total Loss: 0.012599717825651169, \t Rec Loss: 0.004782119765877724,\t KL Loss: 7.81759786605835\n",
            "It 17900: Total Loss: 0.013060725294053555, \t Rec Loss: 0.004832559265196323,\t KL Loss: 8.228165626525879\n",
            "It 18000: Total Loss: 0.012296464294195175, \t Rec Loss: 0.0047272322699427605,\t KL Loss: 7.569231986999512\n",
            "It 18100: Total Loss: 0.012226174585521221, \t Rec Loss: 0.004372771829366684,\t KL Loss: 7.853402614593506\n",
            "It 18200: Total Loss: 0.012625272385776043, \t Rec Loss: 0.004548053257167339,\t KL Loss: 8.077219009399414\n",
            "It 18300: Total Loss: 0.01299494318664074, \t Rec Loss: 0.004884805995970964,\t KL Loss: 8.110136032104492\n",
            "It 18400: Total Loss: 0.012391641736030579, \t Rec Loss: 0.004586010240018368,\t KL Loss: 7.805630683898926\n",
            "It 18500: Total Loss: 0.012118611484766006, \t Rec Loss: 0.004345598164945841,\t KL Loss: 7.773013114929199\n",
            "It 18600: Total Loss: 0.013057161122560501, \t Rec Loss: 0.005084518808871508,\t KL Loss: 7.972641468048096\n",
            "It 18700: Total Loss: 0.012249840423464775, \t Rec Loss: 0.004474402870982885,\t KL Loss: 7.775437831878662\n",
            "It 18800: Total Loss: 0.012374636717140675, \t Rec Loss: 0.004454471170902252,\t KL Loss: 7.920165538787842\n",
            "It 18900: Total Loss: 0.012015888467431068, \t Rec Loss: 0.004185402300208807,\t KL Loss: 7.830486297607422\n",
            "It 19000: Total Loss: 0.012280229479074478, \t Rec Loss: 0.004719212651252747,\t KL Loss: 7.56101655960083\n",
            "Epoch 8/20\n",
            "It 19100: Total Loss: 0.013194847851991653, \t Rec Loss: 0.004962267819792032,\t KL Loss: 8.232580184936523\n",
            "It 19200: Total Loss: 0.013529695570468903, \t Rec Loss: 0.005831632763147354,\t KL Loss: 7.698062896728516\n",
            "It 19300: Total Loss: 0.012311890721321106, \t Rec Loss: 0.004540789872407913,\t KL Loss: 7.771100997924805\n",
            "It 19400: Total Loss: 0.01215299591422081, \t Rec Loss: 0.004404264502227306,\t KL Loss: 7.748730659484863\n",
            "It 19500: Total Loss: 0.012691158801317215, \t Rec Loss: 0.00463805953040719,\t KL Loss: 8.053098678588867\n",
            "It 19600: Total Loss: 0.01194702833890915, \t Rec Loss: 0.004078222904354334,\t KL Loss: 7.868805408477783\n",
            "It 19700: Total Loss: 0.012987812981009483, \t Rec Loss: 0.004916748497635126,\t KL Loss: 8.071063995361328\n",
            "It 19800: Total Loss: 0.0119874756783247, \t Rec Loss: 0.004250367637723684,\t KL Loss: 7.73710823059082\n",
            "It 19900: Total Loss: 0.011964302510023117, \t Rec Loss: 0.004239021334797144,\t KL Loss: 7.72528076171875\n",
            "It 20000: Total Loss: 0.011638794094324112, \t Rec Loss: 0.0035916080232709646,\t KL Loss: 8.047185897827148\n",
            "It 20100: Total Loss: 0.012040789239108562, \t Rec Loss: 0.0039147669449448586,\t KL Loss: 8.126022338867188\n",
            "It 20200: Total Loss: 0.012098707258701324, \t Rec Loss: 0.004291508346796036,\t KL Loss: 7.807198524475098\n",
            "It 20300: Total Loss: 0.01237477920949459, \t Rec Loss: 0.003930528648197651,\t KL Loss: 8.444250106811523\n",
            "It 20400: Total Loss: 0.012559665367007256, \t Rec Loss: 0.004732078872621059,\t KL Loss: 7.8275861740112305\n",
            "It 20500: Total Loss: 0.012188674882054329, \t Rec Loss: 0.004020246211439371,\t KL Loss: 8.168428421020508\n",
            "It 20600: Total Loss: 0.012495663948357105, \t Rec Loss: 0.004173244349658489,\t KL Loss: 8.322419166564941\n",
            "It 20700: Total Loss: 0.012730548158288002, \t Rec Loss: 0.004549434874206781,\t KL Loss: 8.181112289428711\n",
            "It 20800: Total Loss: 0.012257492169737816, \t Rec Loss: 0.004031526390463114,\t KL Loss: 8.225964546203613\n",
            "It 20900: Total Loss: 0.012963450513780117, \t Rec Loss: 0.004671383649110794,\t KL Loss: 8.29206657409668\n",
            "It 21000: Total Loss: 0.011838153004646301, \t Rec Loss: 0.004195752087980509,\t KL Loss: 7.642400741577148\n",
            "It 21100: Total Loss: 0.012249847874045372, \t Rec Loss: 0.004284089896827936,\t KL Loss: 7.965757369995117\n",
            "It 21200: Total Loss: 0.013324180617928505, \t Rec Loss: 0.004908852279186249,\t KL Loss: 8.415328025817871\n",
            "It 21300: Total Loss: 0.012535540387034416, \t Rec Loss: 0.004349750000983477,\t KL Loss: 8.185790061950684\n",
            "It 21400: Total Loss: 0.011744586750864983, \t Rec Loss: 0.00398116372525692,\t KL Loss: 7.76342248916626\n",
            "It 21500: Total Loss: 0.01270776055753231, \t Rec Loss: 0.00471541890874505,\t KL Loss: 7.992340564727783\n",
            "It 21600: Total Loss: 0.012011541053652763, \t Rec Loss: 0.004046003334224224,\t KL Loss: 7.9655375480651855\n",
            "It 21700: Total Loss: 0.01323791779577732, \t Rec Loss: 0.004863348323851824,\t KL Loss: 8.3745698928833\n",
            "It 21800: Total Loss: 0.013675999827682972, \t Rec Loss: 0.005492016673088074,\t KL Loss: 8.183982849121094\n",
            "Epoch 9/20\n",
            "It 21900: Total Loss: 0.01232290081679821, \t Rec Loss: 0.004292586352676153,\t KL Loss: 8.030314445495605\n",
            "It 22000: Total Loss: 0.011795178055763245, \t Rec Loss: 0.0035628548357635736,\t KL Loss: 8.232322692871094\n",
            "It 22100: Total Loss: 0.012658113613724709, \t Rec Loss: 0.0040861740708351135,\t KL Loss: 8.571939468383789\n",
            "It 22200: Total Loss: 0.012482866644859314, \t Rec Loss: 0.00446506729349494,\t KL Loss: 8.017799377441406\n",
            "It 22300: Total Loss: 0.013327440246939659, \t Rec Loss: 0.005258024204522371,\t KL Loss: 8.069416046142578\n",
            "It 22400: Total Loss: 0.012128394097089767, \t Rec Loss: 0.004208012949675322,\t KL Loss: 7.920381546020508\n",
            "It 22500: Total Loss: 0.0116325868293643, \t Rec Loss: 0.0033515598624944687,\t KL Loss: 8.281026840209961\n",
            "It 22600: Total Loss: 0.012797472067177296, \t Rec Loss: 0.004563508555293083,\t KL Loss: 8.233963012695312\n",
            "It 22700: Total Loss: 0.014088778756558895, \t Rec Loss: 0.005669827573001385,\t KL Loss: 8.418951034545898\n",
            "It 22800: Total Loss: 0.01336660422384739, \t Rec Loss: 0.005452387500554323,\t KL Loss: 7.914215564727783\n",
            "It 22900: Total Loss: 0.011640811339020729, \t Rec Loss: 0.004002857021987438,\t KL Loss: 7.637954235076904\n",
            "It 23000: Total Loss: 0.01207255944609642, \t Rec Loss: 0.003942777868360281,\t KL Loss: 8.129781723022461\n",
            "It 23100: Total Loss: 0.014349100179970264, \t Rec Loss: 0.006040092557668686,\t KL Loss: 8.30900764465332\n",
            "It 23200: Total Loss: 0.012522886507213116, \t Rec Loss: 0.00458137784153223,\t KL Loss: 7.9415082931518555\n",
            "It 23300: Total Loss: 0.01208227314054966, \t Rec Loss: 0.0043883053585886955,\t KL Loss: 7.693967819213867\n",
            "It 23400: Total Loss: 0.012376850470900536, \t Rec Loss: 0.004686472937464714,\t KL Loss: 7.690377235412598\n",
            "It 23500: Total Loss: 0.012417599558830261, \t Rec Loss: 0.004821541253477335,\t KL Loss: 7.596057415008545\n",
            "It 23600: Total Loss: 0.011560660786926746, \t Rec Loss: 0.003644277574494481,\t KL Loss: 7.916382789611816\n",
            "It 23700: Total Loss: 0.0121182044968009, \t Rec Loss: 0.0038179783150553703,\t KL Loss: 8.300226211547852\n",
            "It 23800: Total Loss: 0.012149889953434467, \t Rec Loss: 0.004610886797308922,\t KL Loss: 7.539002895355225\n",
            "It 23900: Total Loss: 0.012171408161520958, \t Rec Loss: 0.004341682884842157,\t KL Loss: 7.82972526550293\n",
            "It 24000: Total Loss: 0.012667128816246986, \t Rec Loss: 0.004643935710191727,\t KL Loss: 8.023192405700684\n",
            "It 24100: Total Loss: 0.012958514504134655, \t Rec Loss: 0.004565219394862652,\t KL Loss: 8.393294334411621\n",
            "It 24200: Total Loss: 0.012692077085375786, \t Rec Loss: 0.004190313164144754,\t KL Loss: 8.501764297485352\n",
            "It 24300: Total Loss: 0.012459322810173035, \t Rec Loss: 0.003956788219511509,\t KL Loss: 8.502533912658691\n",
            "It 24400: Total Loss: 0.012134402990341187, \t Rec Loss: 0.0045243920758366585,\t KL Loss: 7.610010147094727\n",
            "It 24500: Total Loss: 0.01146285142749548, \t Rec Loss: 0.0036747653502970934,\t KL Loss: 7.7880859375\n",
            "Epoch 10/20\n",
            "It 24600: Total Loss: 0.012368205934762955, \t Rec Loss: 0.004183836746960878,\t KL Loss: 8.184368133544922\n",
            "It 24700: Total Loss: 0.012136591598391533, \t Rec Loss: 0.00459463894367218,\t KL Loss: 7.541951656341553\n",
            "It 24800: Total Loss: 0.0119086392223835, \t Rec Loss: 0.004338568542152643,\t KL Loss: 7.570069789886475\n",
            "It 24900: Total Loss: 0.012191306799650192, \t Rec Loss: 0.004104056395590305,\t KL Loss: 8.087249755859375\n",
            "It 25000: Total Loss: 0.012734021991491318, \t Rec Loss: 0.004730702377855778,\t KL Loss: 8.003318786621094\n",
            "It 25100: Total Loss: 0.011908826418220997, \t Rec Loss: 0.003802632912993431,\t KL Loss: 8.106193542480469\n",
            "It 25200: Total Loss: 0.012185543775558472, \t Rec Loss: 0.003920069430023432,\t KL Loss: 8.265474319458008\n",
            "It 25300: Total Loss: 0.011933635920286179, \t Rec Loss: 0.004103414248675108,\t KL Loss: 7.830221176147461\n",
            "It 25400: Total Loss: 0.012474749237298965, \t Rec Loss: 0.0044509232975542545,\t KL Loss: 8.023824691772461\n",
            "It 25500: Total Loss: 0.0127493217587471, \t Rec Loss: 0.004470237530767918,\t KL Loss: 8.279084205627441\n",
            "It 25600: Total Loss: 0.0131980050355196, \t Rec Loss: 0.004599767737090588,\t KL Loss: 8.598237037658691\n",
            "It 25700: Total Loss: 0.01250264048576355, \t Rec Loss: 0.004434205591678619,\t KL Loss: 8.068434715270996\n",
            "It 25800: Total Loss: 0.012884465977549553, \t Rec Loss: 0.0041478159837424755,\t KL Loss: 8.736650466918945\n",
            "It 25900: Total Loss: 0.012367788702249527, \t Rec Loss: 0.004358320962637663,\t KL Loss: 8.009467124938965\n",
            "It 26000: Total Loss: 0.01137563493102789, \t Rec Loss: 0.0036322129890322685,\t KL Loss: 7.74342155456543\n",
            "It 26100: Total Loss: 0.013095426373183727, \t Rec Loss: 0.004384973086416721,\t KL Loss: 8.710453033447266\n",
            "It 26200: Total Loss: 0.012202843092381954, \t Rec Loss: 0.004507546313107014,\t KL Loss: 7.695296287536621\n",
            "It 26300: Total Loss: 0.012781903147697449, \t Rec Loss: 0.0048668826930224895,\t KL Loss: 7.91502046585083\n",
            "It 26400: Total Loss: 0.012412958778440952, \t Rec Loss: 0.004262321628630161,\t KL Loss: 8.150636672973633\n",
            "It 26500: Total Loss: 0.012205684557557106, \t Rec Loss: 0.0041907853446900845,\t KL Loss: 8.014899253845215\n",
            "It 26600: Total Loss: 0.012738116085529327, \t Rec Loss: 0.004765473306179047,\t KL Loss: 7.972642421722412\n",
            "It 26700: Total Loss: 0.011921018362045288, \t Rec Loss: 0.0039379834197461605,\t KL Loss: 7.983034133911133\n",
            "It 26800: Total Loss: 0.01275497768074274, \t Rec Loss: 0.004772750660777092,\t KL Loss: 7.982226848602295\n",
            "It 26900: Total Loss: 0.012687296606600285, \t Rec Loss: 0.004742317833006382,\t KL Loss: 7.9449782371521\n",
            "It 27000: Total Loss: 0.012541724368929863, \t Rec Loss: 0.003996778279542923,\t KL Loss: 8.54494571685791\n",
            "It 27100: Total Loss: 0.012173613533377647, \t Rec Loss: 0.004106822889298201,\t KL Loss: 8.066789627075195\n",
            "It 27200: Total Loss: 0.011623596772551537, \t Rec Loss: 0.0040345932357013226,\t KL Loss: 7.589003562927246\n",
            "Epoch 11/20\n",
            "It 27300: Total Loss: 0.011974141001701355, \t Rec Loss: 0.004623768851161003,\t KL Loss: 7.350371360778809\n",
            "It 27400: Total Loss: 0.011263210326433182, \t Rec Loss: 0.0037222830578684807,\t KL Loss: 7.540927410125732\n",
            "It 27500: Total Loss: 0.01284634880721569, \t Rec Loss: 0.004852945916354656,\t KL Loss: 7.99340295791626\n",
            "It 27600: Total Loss: 0.012551186606287956, \t Rec Loss: 0.0044510276056826115,\t KL Loss: 8.100157737731934\n",
            "It 27700: Total Loss: 0.012286404147744179, \t Rec Loss: 0.004058775492012501,\t KL Loss: 8.227628707885742\n",
            "It 27800: Total Loss: 0.012016087770462036, \t Rec Loss: 0.004051344934850931,\t KL Loss: 7.964742660522461\n",
            "It 27900: Total Loss: 0.012725697830319405, \t Rec Loss: 0.0049432688392698765,\t KL Loss: 7.782428741455078\n",
            "It 28000: Total Loss: 0.01176538411527872, \t Rec Loss: 0.004036562982946634,\t KL Loss: 7.72882080078125\n",
            "It 28100: Total Loss: 0.01226058416068554, \t Rec Loss: 0.003972915932536125,\t KL Loss: 8.287668228149414\n",
            "It 28200: Total Loss: 0.01273336261510849, \t Rec Loss: 0.0049461182206869125,\t KL Loss: 7.787243843078613\n",
            "It 28300: Total Loss: 0.012053059414029121, \t Rec Loss: 0.004179174546152353,\t KL Loss: 7.873885154724121\n",
            "It 28400: Total Loss: 0.012905487790703773, \t Rec Loss: 0.0044485898688435555,\t KL Loss: 8.456897735595703\n",
            "It 28500: Total Loss: 0.011749137192964554, \t Rec Loss: 0.003990898374468088,\t KL Loss: 7.758238792419434\n",
            "It 28600: Total Loss: 0.01268334873020649, \t Rec Loss: 0.004603436682373285,\t KL Loss: 8.079911231994629\n",
            "It 28700: Total Loss: 0.013363922014832497, \t Rec Loss: 0.005768644157797098,\t KL Loss: 7.595276832580566\n",
            "It 28800: Total Loss: 0.012367483228445053, \t Rec Loss: 0.004323842469602823,\t KL Loss: 8.04364013671875\n",
            "It 28900: Total Loss: 0.011872276663780212, \t Rec Loss: 0.003930025268346071,\t KL Loss: 7.9422502517700195\n",
            "It 29000: Total Loss: 0.012250454165041447, \t Rec Loss: 0.0043061235919594765,\t KL Loss: 7.944330215454102\n",
            "It 29100: Total Loss: 0.011724842712283134, \t Rec Loss: 0.0041237398982048035,\t KL Loss: 7.601101875305176\n",
            "It 29200: Total Loss: 0.012454038485884666, \t Rec Loss: 0.003853039350360632,\t KL Loss: 8.600997924804688\n",
            "It 29300: Total Loss: 0.011425245553255081, \t Rec Loss: 0.004078269004821777,\t KL Loss: 7.346976280212402\n",
            "It 29400: Total Loss: 0.012244205921888351, \t Rec Loss: 0.0043258084915578365,\t KL Loss: 7.918396949768066\n",
            "It 29500: Total Loss: 0.012086087837815285, \t Rec Loss: 0.0042327088303864,\t KL Loss: 7.8533782958984375\n",
            "It 29600: Total Loss: 0.012710129842162132, \t Rec Loss: 0.004376691300421953,\t KL Loss: 8.333438873291016\n",
            "It 29700: Total Loss: 0.011331046000123024, \t Rec Loss: 0.00375958951190114,\t KL Loss: 7.571456432342529\n",
            "It 29800: Total Loss: 0.012548627331852913, \t Rec Loss: 0.0041097248904407024,\t KL Loss: 8.438901901245117\n",
            "It 29900: Total Loss: 0.01140652410686016, \t Rec Loss: 0.0035242470912635326,\t KL Loss: 7.882277011871338\n",
            "Epoch 12/20\n",
            "It 30000: Total Loss: 0.01216310728341341, \t Rec Loss: 0.004129201173782349,\t KL Loss: 8.033905982971191\n",
            "It 30100: Total Loss: 0.011698361486196518, \t Rec Loss: 0.003821625839918852,\t KL Loss: 7.876735687255859\n",
            "It 30200: Total Loss: 0.012174546718597412, \t Rec Loss: 0.003814340103417635,\t KL Loss: 8.360206604003906\n",
            "It 30300: Total Loss: 0.012500849552452564, \t Rec Loss: 0.00438073743134737,\t KL Loss: 8.120111465454102\n",
            "It 30400: Total Loss: 0.01177624799311161, \t Rec Loss: 0.003892274107784033,\t KL Loss: 7.883974075317383\n",
            "It 30500: Total Loss: 0.01335928961634636, \t Rec Loss: 0.005866786930710077,\t KL Loss: 7.492502212524414\n",
            "It 30600: Total Loss: 0.012613428756594658, \t Rec Loss: 0.004450263921171427,\t KL Loss: 8.163164138793945\n",
            "It 30700: Total Loss: 0.01284199021756649, \t Rec Loss: 0.004313623532652855,\t KL Loss: 8.528366088867188\n",
            "It 30800: Total Loss: 0.012463665567338467, \t Rec Loss: 0.00411103293299675,\t KL Loss: 8.352632522583008\n",
            "It 30900: Total Loss: 0.012335003353655338, \t Rec Loss: 0.004095539450645447,\t KL Loss: 8.239463806152344\n",
            "It 31000: Total Loss: 0.012705141678452492, \t Rec Loss: 0.003921697847545147,\t KL Loss: 8.783443450927734\n",
            "It 31100: Total Loss: 0.012514004483819008, \t Rec Loss: 0.0045806230045855045,\t KL Loss: 7.9333815574646\n",
            "It 31200: Total Loss: 0.01179608516395092, \t Rec Loss: 0.004313359502702951,\t KL Loss: 7.482725620269775\n",
            "It 31300: Total Loss: 0.011301649734377861, \t Rec Loss: 0.003453013952821493,\t KL Loss: 7.848636150360107\n",
            "It 31400: Total Loss: 0.012359876185655594, \t Rec Loss: 0.00464273989200592,\t KL Loss: 7.717135429382324\n",
            "It 31500: Total Loss: 0.012172400951385498, \t Rec Loss: 0.004431586712598801,\t KL Loss: 7.740813255310059\n",
            "It 31600: Total Loss: 0.012546603567898273, \t Rec Loss: 0.004148293286561966,\t KL Loss: 8.398309707641602\n",
            "It 31700: Total Loss: 0.011813934892416, \t Rec Loss: 0.003879960160702467,\t KL Loss: 7.9339752197265625\n",
            "It 31800: Total Loss: 0.012083981186151505, \t Rec Loss: 0.003980111796408892,\t KL Loss: 8.10386848449707\n",
            "It 31900: Total Loss: 0.011247413232922554, \t Rec Loss: 0.0032863463275134563,\t KL Loss: 7.961067199707031\n",
            "It 32000: Total Loss: 0.013554476201534271, \t Rec Loss: 0.004856402985751629,\t KL Loss: 8.69807243347168\n",
            "It 32100: Total Loss: 0.012003717944025993, \t Rec Loss: 0.004061311017721891,\t KL Loss: 7.942407131195068\n",
            "It 32200: Total Loss: 0.011160032823681831, \t Rec Loss: 0.003595413640141487,\t KL Loss: 7.564619064331055\n",
            "It 32300: Total Loss: 0.012789683416485786, \t Rec Loss: 0.004599602427333593,\t KL Loss: 8.190080642700195\n",
            "It 32400: Total Loss: 0.011449703946709633, \t Rec Loss: 0.003578688483685255,\t KL Loss: 7.8710150718688965\n",
            "It 32500: Total Loss: 0.012995782308280468, \t Rec Loss: 0.004591383971273899,\t KL Loss: 8.404397964477539\n",
            "It 32600: Total Loss: 0.01266392134130001, \t Rec Loss: 0.00488686328753829,\t KL Loss: 7.77705717086792\n",
            "It 32700: Total Loss: 0.012334284372627735, \t Rec Loss: 0.004199188202619553,\t KL Loss: 8.135095596313477\n",
            "Epoch 13/20\n",
            "It 32800: Total Loss: 0.013266956433653831, \t Rec Loss: 0.00484978873282671,\t KL Loss: 8.417167663574219\n",
            "It 32900: Total Loss: 0.012708216905593872, \t Rec Loss: 0.004233383573591709,\t KL Loss: 8.474832534790039\n",
            "It 33000: Total Loss: 0.012332649901509285, \t Rec Loss: 0.0043098232708871365,\t KL Loss: 8.0228271484375\n",
            "It 33100: Total Loss: 0.012114405632019043, \t Rec Loss: 0.0042757210321724415,\t KL Loss: 7.838683605194092\n",
            "It 33200: Total Loss: 0.012655280530452728, \t Rec Loss: 0.0040792617946863174,\t KL Loss: 8.576018333435059\n",
            "It 33300: Total Loss: 0.012483851984143257, \t Rec Loss: 0.004546710290014744,\t KL Loss: 7.937141418457031\n",
            "It 33400: Total Loss: 0.012285706587135792, \t Rec Loss: 0.0039504580199718475,\t KL Loss: 8.335247993469238\n",
            "It 33500: Total Loss: 0.012761667370796204, \t Rec Loss: 0.004215372260659933,\t KL Loss: 8.546295166015625\n",
            "It 33600: Total Loss: 0.01226088311523199, \t Rec Loss: 0.004059728235006332,\t KL Loss: 8.201154708862305\n",
            "It 33700: Total Loss: 0.012151842005550861, \t Rec Loss: 0.0041394662111997604,\t KL Loss: 8.012375831604004\n",
            "It 33800: Total Loss: 0.011806625872850418, \t Rec Loss: 0.004113500006496906,\t KL Loss: 7.6931257247924805\n",
            "It 33900: Total Loss: 0.01229509525001049, \t Rec Loss: 0.004542300943285227,\t KL Loss: 7.75279426574707\n",
            "It 34000: Total Loss: 0.012001311406493187, \t Rec Loss: 0.0038537171203643084,\t KL Loss: 8.147594451904297\n",
            "It 34100: Total Loss: 0.01229821890592575, \t Rec Loss: 0.004106367938220501,\t KL Loss: 8.191850662231445\n",
            "It 34200: Total Loss: 0.01288018748164177, \t Rec Loss: 0.004396712873131037,\t KL Loss: 8.483473777770996\n",
            "It 34300: Total Loss: 0.011699909344315529, \t Rec Loss: 0.0035240871366113424,\t KL Loss: 8.175821304321289\n",
            "It 34400: Total Loss: 0.012503521516919136, \t Rec Loss: 0.0046411617659032345,\t KL Loss: 7.862359046936035\n",
            "It 34500: Total Loss: 0.011881865561008453, \t Rec Loss: 0.003954319749027491,\t KL Loss: 7.927545547485352\n",
            "It 34600: Total Loss: 0.012663612142205238, \t Rec Loss: 0.00475875660777092,\t KL Loss: 7.904855251312256\n",
            "It 34700: Total Loss: 0.012932965531945229, \t Rec Loss: 0.004363623913377523,\t KL Loss: 8.569341659545898\n",
            "It 34800: Total Loss: 0.012669546529650688, \t Rec Loss: 0.004060355480760336,\t KL Loss: 8.609190940856934\n",
            "It 34900: Total Loss: 0.01212363038212061, \t Rec Loss: 0.0041905054822564125,\t KL Loss: 7.93312406539917\n",
            "It 35000: Total Loss: 0.011754840612411499, \t Rec Loss: 0.004222666844725609,\t KL Loss: 7.5321736335754395\n",
            "It 35100: Total Loss: 0.012149326503276825, \t Rec Loss: 0.004193759523332119,\t KL Loss: 7.955566883087158\n",
            "It 35200: Total Loss: 0.011848092079162598, \t Rec Loss: 0.004080271814018488,\t KL Loss: 7.767820358276367\n",
            "It 35300: Total Loss: 0.011889666318893433, \t Rec Loss: 0.004123741295188665,\t KL Loss: 7.765923976898193\n",
            "It 35400: Total Loss: 0.013108095154166222, \t Rec Loss: 0.004904917906969786,\t KL Loss: 8.203176498413086\n",
            "Epoch 14/20\n",
            "It 35500: Total Loss: 0.012384023517370224, \t Rec Loss: 0.004165736027061939,\t KL Loss: 8.218287467956543\n",
            "It 35600: Total Loss: 0.012774806469678879, \t Rec Loss: 0.0049947635270655155,\t KL Loss: 7.7800421714782715\n",
            "It 35700: Total Loss: 0.0116675840690732, \t Rec Loss: 0.003552531125023961,\t KL Loss: 8.115053176879883\n",
            "It 35800: Total Loss: 0.012272905558347702, \t Rec Loss: 0.004265899304300547,\t KL Loss: 8.00700569152832\n",
            "It 35900: Total Loss: 0.012326272204518318, \t Rec Loss: 0.0043527912348508835,\t KL Loss: 7.973480701446533\n",
            "It 36000: Total Loss: 0.01221201941370964, \t Rec Loss: 0.004100583493709564,\t KL Loss: 8.111435890197754\n",
            "It 36100: Total Loss: 0.012013349682092667, \t Rec Loss: 0.0038298661820590496,\t KL Loss: 8.183483123779297\n",
            "It 36200: Total Loss: 0.013219702988862991, \t Rec Loss: 0.005184382200241089,\t KL Loss: 8.035320281982422\n",
            "It 36300: Total Loss: 0.012883555144071579, \t Rec Loss: 0.004699230194091797,\t KL Loss: 8.184324264526367\n",
            "It 36400: Total Loss: 0.011488651856780052, \t Rec Loss: 0.003972815815359354,\t KL Loss: 7.515835762023926\n",
            "It 36500: Total Loss: 0.014026222750544548, \t Rec Loss: 0.0056485943496227264,\t KL Loss: 8.377628326416016\n",
            "It 36600: Total Loss: 0.012742673978209496, \t Rec Loss: 0.004638488870114088,\t KL Loss: 8.104185104370117\n",
            "It 36700: Total Loss: 0.0123737258836627, \t Rec Loss: 0.0037507396191358566,\t KL Loss: 8.62298583984375\n",
            "It 36800: Total Loss: 0.012532109394669533, \t Rec Loss: 0.0044864085502922535,\t KL Loss: 8.045700073242188\n",
            "It 36900: Total Loss: 0.012405522167682648, \t Rec Loss: 0.0038734558038413525,\t KL Loss: 8.532066345214844\n",
            "It 37000: Total Loss: 0.012026350013911724, \t Rec Loss: 0.0035354217980057,\t KL Loss: 8.490927696228027\n",
            "It 37100: Total Loss: 0.012731986120343208, \t Rec Loss: 0.004765720572322607,\t KL Loss: 7.9662652015686035\n",
            "It 37200: Total Loss: 0.011908762156963348, \t Rec Loss: 0.00424238434061408,\t KL Loss: 7.666378021240234\n",
            "It 37300: Total Loss: 0.01196194440126419, \t Rec Loss: 0.004616336897015572,\t KL Loss: 7.345606803894043\n",
            "It 37400: Total Loss: 0.011510098353028297, \t Rec Loss: 0.0033792476169764996,\t KL Loss: 8.130849838256836\n",
            "It 37500: Total Loss: 0.012342352420091629, \t Rec Loss: 0.0038601744454354048,\t KL Loss: 8.482177734375\n",
            "It 37600: Total Loss: 0.012389293871819973, \t Rec Loss: 0.004650197923183441,\t KL Loss: 7.739095687866211\n",
            "It 37700: Total Loss: 0.011958373710513115, \t Rec Loss: 0.003989456687122583,\t KL Loss: 7.968916416168213\n",
            "It 37800: Total Loss: 0.012596851214766502, \t Rec Loss: 0.003842767095193267,\t KL Loss: 8.754083633422852\n",
            "It 37900: Total Loss: 0.01252755243331194, \t Rec Loss: 0.00430037546902895,\t KL Loss: 8.227176666259766\n",
            "It 38000: Total Loss: 0.012064572423696518, \t Rec Loss: 0.004068191163241863,\t KL Loss: 7.996380805969238\n",
            "It 38100: Total Loss: 0.011905170977115631, \t Rec Loss: 0.004172961693257093,\t KL Loss: 7.732209205627441\n",
            "Epoch 15/20\n",
            "It 38200: Total Loss: 0.012715691700577736, \t Rec Loss: 0.00448271632194519,\t KL Loss: 8.232975006103516\n",
            "It 38300: Total Loss: 0.012990881688892841, \t Rec Loss: 0.004713729023933411,\t KL Loss: 8.277152061462402\n",
            "It 38400: Total Loss: 0.012816308997571468, \t Rec Loss: 0.004477229900658131,\t KL Loss: 8.339078903198242\n",
            "It 38500: Total Loss: 0.011645842343568802, \t Rec Loss: 0.003901793621480465,\t KL Loss: 7.744048595428467\n",
            "It 38600: Total Loss: 0.01197439432144165, \t Rec Loss: 0.0037546951789408922,\t KL Loss: 8.219698905944824\n",
            "It 38700: Total Loss: 0.011988578364253044, \t Rec Loss: 0.0040817600674927235,\t KL Loss: 7.906817436218262\n",
            "It 38800: Total Loss: 0.01337941363453865, \t Rec Loss: 0.005072454456239939,\t KL Loss: 8.30695915222168\n",
            "It 38900: Total Loss: 0.01207682304084301, \t Rec Loss: 0.003738213097676635,\t KL Loss: 8.33860969543457\n",
            "It 39000: Total Loss: 0.012453475967049599, \t Rec Loss: 0.004294484853744507,\t KL Loss: 8.158990859985352\n",
            "It 39100: Total Loss: 0.012870218604803085, \t Rec Loss: 0.004136072937399149,\t KL Loss: 8.734146118164062\n",
            "It 39200: Total Loss: 0.012043997645378113, \t Rec Loss: 0.003626713529229164,\t KL Loss: 8.41728401184082\n",
            "It 39300: Total Loss: 0.011311588808894157, \t Rec Loss: 0.003937946166843176,\t KL Loss: 7.3736419677734375\n",
            "It 39400: Total Loss: 0.012812696397304535, \t Rec Loss: 0.004741794895380735,\t KL Loss: 8.070900917053223\n",
            "It 39500: Total Loss: 0.012548636645078659, \t Rec Loss: 0.003946727607399225,\t KL Loss: 8.601907730102539\n",
            "It 39600: Total Loss: 0.012604238465428352, \t Rec Loss: 0.0044500879012048244,\t KL Loss: 8.154150009155273\n",
            "It 39700: Total Loss: 0.012108350172638893, \t Rec Loss: 0.004354203585535288,\t KL Loss: 7.754145622253418\n",
            "It 39800: Total Loss: 0.012102929875254631, \t Rec Loss: 0.003778154030442238,\t KL Loss: 8.324775695800781\n",
            "It 39900: Total Loss: 0.01187707856297493, \t Rec Loss: 0.003895554691553116,\t KL Loss: 7.981523513793945\n",
            "It 40000: Total Loss: 0.012701038271188736, \t Rec Loss: 0.004258311353623867,\t KL Loss: 8.442726135253906\n",
            "It 40100: Total Loss: 0.012340057641267776, \t Rec Loss: 0.0046468875370919704,\t KL Loss: 7.693170070648193\n",
            "It 40200: Total Loss: 0.012115604244172573, \t Rec Loss: 0.003978339023888111,\t KL Loss: 8.1372652053833\n",
            "It 40300: Total Loss: 0.01127881184220314, \t Rec Loss: 0.003620586358010769,\t KL Loss: 7.658225059509277\n",
            "It 40400: Total Loss: 0.012035541236400604, \t Rec Loss: 0.004201854113489389,\t KL Loss: 7.8336873054504395\n",
            "It 40500: Total Loss: 0.012408047914505005, \t Rec Loss: 0.004091170616447926,\t KL Loss: 8.316877365112305\n",
            "It 40600: Total Loss: 0.011801608838140965, \t Rec Loss: 0.0036969278007745743,\t KL Loss: 8.104681015014648\n",
            "It 40700: Total Loss: 0.012229740619659424, \t Rec Loss: 0.004354243166744709,\t KL Loss: 7.875497341156006\n",
            "It 40800: Total Loss: 0.01176060177385807, \t Rec Loss: 0.003917980473488569,\t KL Loss: 7.842621803283691\n",
            "It 40900: Total Loss: 0.011727221310138702, \t Rec Loss: 0.0034515096340328455,\t KL Loss: 8.275711059570312\n",
            "Epoch 16/20\n",
            "It 41000: Total Loss: 0.011273127980530262, \t Rec Loss: 0.0038020543288439512,\t KL Loss: 7.471073150634766\n",
            "It 41100: Total Loss: 0.012684110552072525, \t Rec Loss: 0.004575144965201616,\t KL Loss: 8.108965873718262\n",
            "It 41200: Total Loss: 0.012809433974325657, \t Rec Loss: 0.004903541877865791,\t KL Loss: 7.905891418457031\n",
            "It 41300: Total Loss: 0.011769290082156658, \t Rec Loss: 0.004095631185919046,\t KL Loss: 7.67365837097168\n",
            "It 41400: Total Loss: 0.012104349210858345, \t Rec Loss: 0.004060402046889067,\t KL Loss: 8.043947219848633\n",
            "It 41500: Total Loss: 0.01183878630399704, \t Rec Loss: 0.0036210003308951855,\t KL Loss: 8.217784881591797\n",
            "It 41600: Total Loss: 0.011294165626168251, \t Rec Loss: 0.00352089898660779,\t KL Loss: 7.773265838623047\n",
            "It 41700: Total Loss: 0.011660232208669186, \t Rec Loss: 0.003615996800363064,\t KL Loss: 8.044235229492188\n",
            "It 41800: Total Loss: 0.012130488641560078, \t Rec Loss: 0.004239510744810104,\t KL Loss: 7.89097785949707\n",
            "It 41900: Total Loss: 0.012201005592942238, \t Rec Loss: 0.00398603267967701,\t KL Loss: 8.214972496032715\n",
            "It 42000: Total Loss: 0.012370659038424492, \t Rec Loss: 0.0042341467924416065,\t KL Loss: 8.136512756347656\n",
            "It 42100: Total Loss: 0.012134753167629242, \t Rec Loss: 0.0037567629478871822,\t KL Loss: 8.37799072265625\n",
            "It 42200: Total Loss: 0.011783614754676819, \t Rec Loss: 0.004063441883772612,\t KL Loss: 7.720171928405762\n",
            "It 42300: Total Loss: 0.01168953999876976, \t Rec Loss: 0.0038992769550532103,\t KL Loss: 7.790262699127197\n",
            "It 42400: Total Loss: 0.01293427124619484, \t Rec Loss: 0.004531389102339745,\t KL Loss: 8.402881622314453\n",
            "It 42500: Total Loss: 0.012133751064538956, \t Rec Loss: 0.003944162745028734,\t KL Loss: 8.18958854675293\n",
            "It 42600: Total Loss: 0.012632927857339382, \t Rec Loss: 0.004265160299837589,\t KL Loss: 8.367767333984375\n",
            "It 42700: Total Loss: 0.012436946853995323, \t Rec Loss: 0.004481747280806303,\t KL Loss: 7.955198287963867\n",
            "It 42800: Total Loss: 0.01158509124070406, \t Rec Loss: 0.0035209401976317167,\t KL Loss: 8.0641508102417\n",
            "It 42900: Total Loss: 0.013016562908887863, \t Rec Loss: 0.0051531377248466015,\t KL Loss: 7.863424777984619\n",
            "It 43000: Total Loss: 0.012408720329403877, \t Rec Loss: 0.004498360212892294,\t KL Loss: 7.9103593826293945\n",
            "It 43100: Total Loss: 0.011307411827147007, \t Rec Loss: 0.0037354319356381893,\t KL Loss: 7.571979522705078\n",
            "It 43200: Total Loss: 0.012898228131234646, \t Rec Loss: 0.004297222942113876,\t KL Loss: 8.601004600524902\n",
            "It 43300: Total Loss: 0.013205774128437042, \t Rec Loss: 0.0046175201423466206,\t KL Loss: 8.588253021240234\n",
            "It 43400: Total Loss: 0.01180245727300644, \t Rec Loss: 0.00409518601372838,\t KL Loss: 7.707270622253418\n",
            "It 43500: Total Loss: 0.012176822870969772, \t Rec Loss: 0.0042596859857439995,\t KL Loss: 7.917136192321777\n",
            "It 43600: Total Loss: 0.011770566925406456, \t Rec Loss: 0.003657723544165492,\t KL Loss: 8.112842559814453\n",
            "Epoch 17/20\n",
            "It 43700: Total Loss: 0.01178524736315012, \t Rec Loss: 0.003787445602938533,\t KL Loss: 7.997800827026367\n",
            "It 43800: Total Loss: 0.011915947310626507, \t Rec Loss: 0.0040705520659685135,\t KL Loss: 7.845394611358643\n",
            "It 43900: Total Loss: 0.011913440190255642, \t Rec Loss: 0.003935767337679863,\t KL Loss: 7.977672576904297\n",
            "It 44000: Total Loss: 0.012289399281144142, \t Rec Loss: 0.004460414871573448,\t KL Loss: 7.828984260559082\n",
            "It 44100: Total Loss: 0.012507705017924309, \t Rec Loss: 0.004145812708884478,\t KL Loss: 8.361892700195312\n",
            "It 44200: Total Loss: 0.012793396599590778, \t Rec Loss: 0.004689626395702362,\t KL Loss: 8.10377025604248\n",
            "It 44300: Total Loss: 0.011668886058032513, \t Rec Loss: 0.003829991677775979,\t KL Loss: 7.838893890380859\n",
            "It 44400: Total Loss: 0.011723567731678486, \t Rec Loss: 0.0040676589123904705,\t KL Loss: 7.655908584594727\n",
            "It 44500: Total Loss: 0.011412162333726883, \t Rec Loss: 0.003742225468158722,\t KL Loss: 7.669936180114746\n",
            "It 44600: Total Loss: 0.01199526060372591, \t Rec Loss: 0.0038832854479551315,\t KL Loss: 8.111974716186523\n",
            "It 44700: Total Loss: 0.011575628072023392, \t Rec Loss: 0.004036502446979284,\t KL Loss: 7.539125442504883\n",
            "It 44800: Total Loss: 0.011901107616722584, \t Rec Loss: 0.004154668189585209,\t KL Loss: 7.746438980102539\n",
            "It 44900: Total Loss: 0.012507101520895958, \t Rec Loss: 0.004068868700414896,\t KL Loss: 8.438232421875\n",
            "It 45000: Total Loss: 0.011892775073647499, \t Rec Loss: 0.004046651069074869,\t KL Loss: 7.846123218536377\n",
            "It 45100: Total Loss: 0.01213177852332592, \t Rec Loss: 0.004009224474430084,\t KL Loss: 8.122553825378418\n",
            "It 45200: Total Loss: 0.012024519965052605, \t Rec Loss: 0.0040621827356517315,\t KL Loss: 7.962336540222168\n",
            "It 45300: Total Loss: 0.012614083476364613, \t Rec Loss: 0.004453309811651707,\t KL Loss: 8.160773277282715\n",
            "It 45400: Total Loss: 0.012143760919570923, \t Rec Loss: 0.004129428416490555,\t KL Loss: 8.014331817626953\n",
            "It 45500: Total Loss: 0.012122711166739464, \t Rec Loss: 0.004334109369665384,\t KL Loss: 7.788600921630859\n",
            "It 45600: Total Loss: 0.01300714910030365, \t Rec Loss: 0.005313613917678595,\t KL Loss: 7.693534851074219\n",
            "It 45700: Total Loss: 0.012381925247609615, \t Rec Loss: 0.004776454530656338,\t KL Loss: 7.605470180511475\n",
            "It 45800: Total Loss: 0.013281004503369331, \t Rec Loss: 0.005119846668094397,\t KL Loss: 8.161157608032227\n",
            "It 45900: Total Loss: 0.011605505831539631, \t Rec Loss: 0.0034012149553745985,\t KL Loss: 8.204290390014648\n",
            "It 46000: Total Loss: 0.012557875365018845, \t Rec Loss: 0.004791506566107273,\t KL Loss: 7.7663679122924805\n",
            "It 46100: Total Loss: 0.011614607647061348, \t Rec Loss: 0.003966225776821375,\t KL Loss: 7.648382186889648\n",
            "It 46200: Total Loss: 0.012673325836658478, \t Rec Loss: 0.004426255822181702,\t KL Loss: 8.247069358825684\n",
            "It 46300: Total Loss: 0.011915162205696106, \t Rec Loss: 0.0036035790108144283,\t KL Loss: 8.311582565307617\n",
            "Epoch 18/20\n",
            "It 46400: Total Loss: 0.012230271473526955, \t Rec Loss: 0.003980516921728849,\t KL Loss: 8.249753952026367\n",
            "It 46500: Total Loss: 0.011816278100013733, \t Rec Loss: 0.0036785800475627184,\t KL Loss: 8.13769817352295\n",
            "It 46600: Total Loss: 0.011166528798639774, \t Rec Loss: 0.0034397428389638662,\t KL Loss: 7.726785659790039\n",
            "It 46700: Total Loss: 0.012855284847319126, \t Rec Loss: 0.004753326065838337,\t KL Loss: 8.101958274841309\n",
            "It 46800: Total Loss: 0.011818351224064827, \t Rec Loss: 0.0038417845498770475,\t KL Loss: 7.976566314697266\n",
            "It 46900: Total Loss: 0.012227381579577923, \t Rec Loss: 0.004020208492875099,\t KL Loss: 8.207172393798828\n",
            "It 47000: Total Loss: 0.012941544875502586, \t Rec Loss: 0.004565993323922157,\t KL Loss: 8.375551223754883\n",
            "It 47100: Total Loss: 0.012361407279968262, \t Rec Loss: 0.004226751625537872,\t KL Loss: 8.134654998779297\n",
            "It 47200: Total Loss: 0.01148365531116724, \t Rec Loss: 0.0032222073059529066,\t KL Loss: 8.261446952819824\n",
            "It 47300: Total Loss: 0.011627410538494587, \t Rec Loss: 0.003606045851483941,\t KL Loss: 8.021364212036133\n",
            "It 47400: Total Loss: 0.012420368380844593, \t Rec Loss: 0.004475599154829979,\t KL Loss: 7.94476842880249\n",
            "It 47500: Total Loss: 0.012056313455104828, \t Rec Loss: 0.0037124883383512497,\t KL Loss: 8.34382438659668\n",
            "It 47600: Total Loss: 0.012512555345892906, \t Rec Loss: 0.004033665172755718,\t KL Loss: 8.478889465332031\n",
            "It 47700: Total Loss: 0.011949929408729076, \t Rec Loss: 0.003709847340360284,\t KL Loss: 8.240081787109375\n",
            "It 47800: Total Loss: 0.012702876701951027, \t Rec Loss: 0.0045738667249679565,\t KL Loss: 8.129009246826172\n",
            "It 47900: Total Loss: 0.011908624321222305, \t Rec Loss: 0.0034298417158424854,\t KL Loss: 8.478782653808594\n",
            "It 48000: Total Loss: 0.011584476567804813, \t Rec Loss: 0.0037346575409173965,\t KL Loss: 7.849818229675293\n",
            "It 48100: Total Loss: 0.01237802766263485, \t Rec Loss: 0.004561224021017551,\t KL Loss: 7.816802978515625\n",
            "It 48200: Total Loss: 0.011415733024477959, \t Rec Loss: 0.0035871523432433605,\t KL Loss: 7.828580379486084\n",
            "It 48300: Total Loss: 0.012242184951901436, \t Rec Loss: 0.004262953065335751,\t KL Loss: 7.979231834411621\n",
            "It 48400: Total Loss: 0.011203029192984104, \t Rec Loss: 0.003449422540143132,\t KL Loss: 7.753605842590332\n",
            "It 48500: Total Loss: 0.012581583112478256, \t Rec Loss: 0.0050172279588878155,\t KL Loss: 7.56435489654541\n",
            "It 48600: Total Loss: 0.012608866207301617, \t Rec Loss: 0.0042724646627902985,\t KL Loss: 8.336400985717773\n",
            "It 48700: Total Loss: 0.013120139017701149, \t Rec Loss: 0.005381772760301828,\t KL Loss: 7.73836612701416\n",
            "It 48800: Total Loss: 0.012585977092385292, \t Rec Loss: 0.003892946755513549,\t KL Loss: 8.693029403686523\n",
            "It 48900: Total Loss: 0.01316540315747261, \t Rec Loss: 0.004818616900593042,\t KL Loss: 8.346786499023438\n",
            "It 49000: Total Loss: 0.012292886152863503, \t Rec Loss: 0.00428773881867528,\t KL Loss: 8.005146026611328\n",
            "Epoch 19/20\n",
            "It 49100: Total Loss: 0.013401689007878304, \t Rec Loss: 0.005144498776644468,\t KL Loss: 8.257190704345703\n",
            "It 49200: Total Loss: 0.012309848330914974, \t Rec Loss: 0.004347159527242184,\t KL Loss: 7.962688446044922\n",
            "It 49300: Total Loss: 0.011915506795048714, \t Rec Loss: 0.004110023379325867,\t KL Loss: 7.805483341217041\n",
            "It 49400: Total Loss: 0.011760594323277473, \t Rec Loss: 0.0038782444316893816,\t KL Loss: 7.882349967956543\n",
            "It 49500: Total Loss: 0.011857656762003899, \t Rec Loss: 0.003748479066416621,\t KL Loss: 8.109176635742188\n",
            "It 49600: Total Loss: 0.011027377098798752, \t Rec Loss: 0.0032165800221264362,\t KL Loss: 7.810797214508057\n",
            "It 49700: Total Loss: 0.01199413277208805, \t Rec Loss: 0.004304998088628054,\t KL Loss: 7.68913459777832\n",
            "It 49800: Total Loss: 0.01238618977367878, \t Rec Loss: 0.0041326130740344524,\t KL Loss: 8.253576278686523\n",
            "It 49900: Total Loss: 0.01202433928847313, \t Rec Loss: 0.003897622227668762,\t KL Loss: 8.126716613769531\n",
            "It 50000: Total Loss: 0.012391848489642143, \t Rec Loss: 0.004592650569975376,\t KL Loss: 7.799197196960449\n",
            "It 50100: Total Loss: 0.01219401229172945, \t Rec Loss: 0.004022680222988129,\t KL Loss: 8.171331405639648\n",
            "It 50200: Total Loss: 0.012536248192191124, \t Rec Loss: 0.004509045276790857,\t KL Loss: 8.027202606201172\n",
            "It 50300: Total Loss: 0.011885314248502254, \t Rec Loss: 0.004208158235996962,\t KL Loss: 7.677155494689941\n",
            "It 50400: Total Loss: 0.011805388145148754, \t Rec Loss: 0.0036195230204612017,\t KL Loss: 8.18586540222168\n",
            "It 50500: Total Loss: 0.011943334713578224, \t Rec Loss: 0.004029730800539255,\t KL Loss: 7.913602828979492\n",
            "It 50600: Total Loss: 0.011970087885856628, \t Rec Loss: 0.00369612080976367,\t KL Loss: 8.273965835571289\n",
            "It 50700: Total Loss: 0.011866857297718525, \t Rec Loss: 0.003940382041037083,\t KL Loss: 7.926474571228027\n",
            "It 50800: Total Loss: 0.013404201716184616, \t Rec Loss: 0.004968060180544853,\t KL Loss: 8.436141014099121\n",
            "It 50900: Total Loss: 0.01188196986913681, \t Rec Loss: 0.003962063696235418,\t KL Loss: 7.919906139373779\n",
            "It 51000: Total Loss: 0.011934923008084297, \t Rec Loss: 0.003985459450632334,\t KL Loss: 7.949463844299316\n",
            "It 51100: Total Loss: 0.01241337414830923, \t Rec Loss: 0.0038927204441279173,\t KL Loss: 8.520652770996094\n",
            "It 51200: Total Loss: 0.012657292187213898, \t Rec Loss: 0.005010752473026514,\t KL Loss: 7.646539211273193\n",
            "It 51300: Total Loss: 0.012319003231823444, \t Rec Loss: 0.004176209680736065,\t KL Loss: 8.142792701721191\n",
            "It 51400: Total Loss: 0.011886338703334332, \t Rec Loss: 0.003838825738057494,\t KL Loss: 8.04751205444336\n",
            "It 51500: Total Loss: 0.01281790342181921, \t Rec Loss: 0.00503572728484869,\t KL Loss: 7.782175540924072\n",
            "It 51600: Total Loss: 0.01267322339117527, \t Rec Loss: 0.004680285695940256,\t KL Loss: 7.992938041687012\n",
            "It 51700: Total Loss: 0.012025737203657627, \t Rec Loss: 0.003990824334323406,\t KL Loss: 8.034912109375\n",
            "It 51800: Total Loss: 0.012087443843483925, \t Rec Loss: 0.0038374881260097027,\t KL Loss: 8.249956130981445\n",
            "Epoch 20/20\n",
            "It 51900: Total Loss: 0.012835584580898285, \t Rec Loss: 0.004438314586877823,\t KL Loss: 8.397269248962402\n",
            "It 52000: Total Loss: 0.01212034560739994, \t Rec Loss: 0.004352020565420389,\t KL Loss: 7.768324375152588\n",
            "It 52100: Total Loss: 0.011496798135340214, \t Rec Loss: 0.0034288575407117605,\t KL Loss: 8.067939758300781\n",
            "It 52200: Total Loss: 0.012591641396284103, \t Rec Loss: 0.0042605274356901646,\t KL Loss: 8.3311128616333\n",
            "It 52300: Total Loss: 0.011470123194158077, \t Rec Loss: 0.00349617563188076,\t KL Loss: 7.973947525024414\n",
            "It 52400: Total Loss: 0.011750671081244946, \t Rec Loss: 0.0037272004410624504,\t KL Loss: 8.023469924926758\n",
            "It 52500: Total Loss: 0.01228133775293827, \t Rec Loss: 0.0038206856697797775,\t KL Loss: 8.460651397705078\n",
            "It 52600: Total Loss: 0.012441866099834442, \t Rec Loss: 0.0043028839863836765,\t KL Loss: 8.138980865478516\n",
            "It 52700: Total Loss: 0.011459083296358585, \t Rec Loss: 0.003824317827820778,\t KL Loss: 7.634765148162842\n",
            "It 52800: Total Loss: 0.012377237901091576, \t Rec Loss: 0.004551728721708059,\t KL Loss: 7.8255085945129395\n",
            "It 52900: Total Loss: 0.011798148974776268, \t Rec Loss: 0.003758301492780447,\t KL Loss: 8.039846420288086\n",
            "It 53000: Total Loss: 0.011874500662088394, \t Rec Loss: 0.0037891878746449947,\t KL Loss: 8.085311889648438\n",
            "It 53100: Total Loss: 0.011909588240087032, \t Rec Loss: 0.0036946593318134546,\t KL Loss: 8.21492862701416\n",
            "It 53200: Total Loss: 0.012283626943826675, \t Rec Loss: 0.004619082901626825,\t KL Loss: 7.664543151855469\n",
            "It 53300: Total Loss: 0.012046687304973602, \t Rec Loss: 0.004112218506634235,\t KL Loss: 7.9344682693481445\n",
            "It 53400: Total Loss: 0.013397116214036942, \t Rec Loss: 0.00504751643165946,\t KL Loss: 8.349599838256836\n",
            "It 53500: Total Loss: 0.012062476016581059, \t Rec Loss: 0.0035024043172597885,\t KL Loss: 8.560070991516113\n",
            "It 53600: Total Loss: 0.012013997882604599, \t Rec Loss: 0.003957071807235479,\t KL Loss: 8.056925773620605\n",
            "It 53700: Total Loss: 0.011896966956555843, \t Rec Loss: 0.0034456781577318907,\t KL Loss: 8.451288223266602\n",
            "It 53800: Total Loss: 0.012389929965138435, \t Rec Loss: 0.0047081708908081055,\t KL Loss: 7.681758880615234\n",
            "It 53900: Total Loss: 0.012057580053806305, \t Rec Loss: 0.003640851704403758,\t KL Loss: 8.416728019714355\n",
            "It 54000: Total Loss: 0.012376070953905582, \t Rec Loss: 0.0038405496161431074,\t KL Loss: 8.535520553588867\n",
            "It 54100: Total Loss: 0.012052198871970177, \t Rec Loss: 0.004393552429974079,\t KL Loss: 7.6586456298828125\n",
            "It 54200: Total Loss: 0.012175166979432106, \t Rec Loss: 0.004500675946474075,\t KL Loss: 7.674490451812744\n",
            "It 54300: Total Loss: 0.011970159597694874, \t Rec Loss: 0.0042574042454361916,\t KL Loss: 7.71275520324707\n",
            "It 54400: Total Loss: 0.012328961864113808, \t Rec Loss: 0.004198387265205383,\t KL Loss: 8.130574226379395\n",
            "It 54500: Total Loss: 0.01212674006819725, \t Rec Loss: 0.003971222788095474,\t KL Loss: 8.155516624450684\n",
            "Done!\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-087d63444518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/vae.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/vae.p'"
          ]
        }
      ],
      "source": [
        "feats_df = load_data(filepaths)\n",
        "feats = preprocessing_features(feats_df)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "batch_size = 64\n",
        "epoch = 20\n",
        "beta = 0.001\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(feats, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "model = LinearVAE(latent_dim=150, beta=beta).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(),lr=1e-3)          # create optimizer instance\n",
        "# criterion = torch.nn.MSELoss()\n",
        "\n",
        "train_it = 0\n",
        "for ep in range(epoch):\n",
        "    print(f'Epoch {ep+1}/{epoch}')\n",
        "    for sample_img in data_loader:\n",
        "        # print(sample_img.float().shape)\n",
        "        opt.zero_grad()\n",
        "        rec, mu, log_var = model.forward(sample_img.float().to(device))\n",
        "        total_loss, losses = model.loss(sample_img.float().to(device), rec, mu, log_var)\n",
        "        total_loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        if train_it % 100 == 0:\n",
        "            print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n",
        "        .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n",
        "        train_it += 1\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YDjRVyqGQRzY"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), model_path+'/vae.p')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Data/MPD_Large//audio_features.txt..\n",
            "(70229, 11) float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.62246964, 0.11411411, 0.72727273, ..., 0.16616617, 0.40423387,\n",
              "        0.42732022],\n",
              "       [0.39473684, 0.31031031, 0.45454545, ..., 0.37937938, 0.58870968,\n",
              "        0.74964421],\n",
              "       [0.5111336 , 0.25825826, 1.        , ..., 0.12012012, 0.64616935,\n",
              "        0.59545437],\n",
              "       ...,\n",
              "       [0.67307692, 0.83783784, 1.        , ..., 0.41941942, 0.57056452,\n",
              "        0.35915692],\n",
              "       [0.55060729, 0.86086086, 0.36363636, ..., 0.37637638, 0.16229839,\n",
              "        0.49420817],\n",
              "       [0.66396761, 0.11311311, 0.18181818, ..., 0.07717718, 0.31350806,\n",
              "        0.33188485]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_path = 'Data/MPD_Large/'\n",
        "feats_df = load_data([data_path + '/audio_features.txt'])\n",
        "feats = preprocessing_features(feats_df)\n",
        "feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = LinearVAE(latent_dim=150, beta=0.01).to(device)\n",
        "model.load_state_dict(torch.load(model_path+'/vae.p'))\n",
        "z, mu, log_var = model.encode(torch.Tensor(feats).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(z, open(data_path + '/audio_embeddings.p','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by54wO_MSNYV"
      },
      "source": [
        "### Fine-tuning with genre classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o7pIwVtXSMZ0"
      },
      "outputs": [],
      "source": [
        "from genre_utils import *\n",
        "\n",
        "data_path = 'data.txt'\n",
        "features_path = 'audio_features.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WpwFkop4Svgv"
      },
      "outputs": [],
      "source": [
        "class EncoderFC(nn.Module):\n",
        "    def __init__(self, latent_dim, n_class, encoder_weights=None, finetune=True):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        \n",
        "        if encoder_weights:\n",
        "            self.encoder.load_state_dict(encoder_weights)\n",
        "            \n",
        "        if finetune:\n",
        "            self.encoder.train()\n",
        "        else:\n",
        "            self.encoder.eval()\n",
        "            \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(latent_dim, 100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(100, n_class)\n",
        "        )\n",
        "        self.sigm = nn.Sigmoid()\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        "    \n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.reshape(-1, 2, self.latent_dim)\n",
        "        mu = x[:, 0, :] # the first feature values as mean\n",
        "        log_var = x[:, 1, :] # the other feature values as variance\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return z\n",
        " \n",
        "    def forward(self, x):\n",
        "        # encoding\n",
        "        z = self.encode(x)\n",
        "        logits = self.classifier(z)\n",
        "        return self.sigm(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcvyKygzQvBm",
        "outputId": "cfb5f5f7-4d4d-49ed-ac27-f622b51155b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load data...\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "data = pd.read_csv(data_path,sep='\\t')\n",
        "assert 'spotify_id' in data.columns\n",
        "assert 'genre' in data.columns, 'need genre column where each song genre is in a|b|c|d format'\n",
        "data.spotify_id.astype(str)\n",
        "feats_df = pd.read_csv(features_path,sep='\\t')\n",
        "print(len(data),len(feats_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPTiVgfLT4dY",
        "outputId": "d6944c48-64a3-4251-cf83-69ec1a77d266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merged length: 89156\n",
            "(89156, 11) float64\n",
            "{'unknown': 0, 'rock': 1, 'metal': 2, 'pop': 3, 'punk': 4, 'alternative': 5, 'post': 6, 'folk': 7, 'jazz': 8, 'rap': 9, 'electro': 10, 'soul': 11, 'melodic': 12, 'experimental': 13, 'death': 14, 'funk': 15, 'hop': 16, 'christian': 17, 'industrial': 18, 'indie': 19, 'hardcore': 20, 'house': 21, 'power': 22, 'noise': 23, 'new': 24, 'art': 25, 'progressive': 26, 'trap': 27, 'dance': 28, 'music': 29, 'hip': 30}\n",
            "(89156, 31)\n"
          ]
        }
      ],
      "source": [
        "# preprocess features, genres\n",
        "feats_df.id.astype(str)\n",
        "feats_df = feats_df[['id'] + feature_columns]\n",
        "feats_df = feats_df.rename(columns={'id':'spotify_id'})\n",
        "data = merge_df(data, feats_df)\n",
        "feats = preprocessing_features(data) \n",
        "# print(data)\n",
        "genres = get_genre_from_df(data, sep='|')\n",
        "genre_list = get_genre_list(genres)\n",
        "genre_onehot = create_multilabel_onehot(genres,genre_list)\n",
        "# print(len(genres),len(genre_list))\n",
        "# assert len(feats) == len(genre_onehot), f'{len(feats)} != {len(genre_onehot)}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3lNr6KUQXU3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "def calculate_metrics(pred, target, threshold=0.5):\n",
        "    pred = np.array(pred >= threshold, dtype=float)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true=target, y_pred=pred)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD-DkksZT5-T",
        "outputId": "07373ec8-8394-4a20-eb50-6909727a6d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Load model...\n",
            "Epoch 1/5\n",
            "It 0: Loss: 0.7155431509017944\n",
            "It 1000: Loss: 0.15980269014835358\n",
            "0.31477410381802684\n",
            "Epoch 2/5\n",
            "It 2000: Loss: 0.13862667977809906\n",
            "0.3210215801516443\n",
            "Epoch 3/5\n",
            "It 3000: Loss: 0.1741984784603119\n",
            "It 4000: Loss: 0.129915252327919\n",
            "0.32249091480102293\n",
            "Epoch 4/5\n",
            "It 5000: Loss: 0.15029728412628174\n",
            "0.32201983040961907\n",
            "Epoch 5/5\n",
            "It 6000: Loss: 0.14682887494564056\n",
            "0.32360132800933195\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "epoch = 5\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Load model\n",
        "# Load VAE encoder weights\n",
        "print('Load model...')\n",
        "model_vae = LinearVAE(latent_dim=150, beta=beta).to(device)\n",
        "model_vae.load_state_dict(torch.load(model_path+'/vae.p'))\n",
        "model_ft = EncoderFC(150, len(genre_list), model_vae.encoder.state_dict())\n",
        "model_ft.to(device)\n",
        "model_ft.train()\n",
        "\n",
        "data_loader_ft = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(feats),torch.Tensor(genre_onehot)), batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "opt = torch.optim.Adam(model_ft.parameters(),lr=1e-3)          # create optimizer instance\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "train_it = 0\n",
        "results = []\n",
        "for ep in range(epoch):\n",
        "    print(f'Epoch {ep+1}/{epoch}')\n",
        "    preds = []\n",
        "    trues = []\n",
        "    for batch_x, batch_y in data_loader_ft:\n",
        "        opt.zero_grad()\n",
        "        output = model_ft.forward(batch_x.float().to(device))\n",
        "        rec_loss = criterion(output, batch_y.float().to(device))\n",
        "        rec_loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        preds.extend(output.cpu().detach().numpy())\n",
        "        trues.extend(batch_y.cpu().detach().numpy())\n",
        "        \n",
        "        if train_it % 1000 == 0:\n",
        "            print(\"It {}: Loss: {}\".format(train_it, rec_loss))\n",
        "        train_it += 1\n",
        "    \n",
        "    result = calculate_metrics(np.array(preds), np.array(trues))\n",
        "    print(result['accuracy'])\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M4_Kgg5zT-5z"
      },
      "outputs": [],
      "source": [
        "torch.save(model_ft.state_dict(), model_path+'/audio_ft.p')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1S-m5qPUSiK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "audio_embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
